{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3321022",
   "metadata": {},
   "source": [
    "\n",
    "# GPT-2 JAX Implementation\n",
    "\n",
    "This notebook demonstrates how to use GPT-2 with JAX for efficient text generation.\n",
    "It includes model loading, inference, and text generation using JAX-based tensor computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0976b88",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca1fa7",
   "metadata": {},
   "source": [
    "## Downloading the Dataset  \n",
    "We fetch a sample text dataset (`tinyshakespeare`) from GitHub.  \n",
    "This dataset will be used to train and evaluate the GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3309c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "631acac9",
    "outputId": "2f1d5e71-0656-496a-9cc8-4073597c6bef"
   },
   "outputs": [],
   "source": [
    "# Download dataset (Shakespeare text) for model training\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53835fc8",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "This section imports all necessary libraries for implementing a GPT-2 model using JAX.  \n",
    "- `jax` is used for accelerated computations with automatic differentiation.  \n",
    "- `optax` provides optimization utilities for training.  \n",
    "- `flax.linen` is a neural network module from Flax, a JAX-native deep learning framework.  \n",
    "- `jax.numpy` is JAXâ€™s version of NumPy for tensor operations.  \n",
    "- Additional libraries help with model training, configuration, and utility functions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7499a1",
   "metadata": {
    "id": "8qohV72gSRbO"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for JAX-based training\n",
    "import jax\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "from typing import Any, Callable, Dict, Optional, Tuple\n",
    "from flax.training import train_state\n",
    "from functools import partial\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e23a5a",
   "metadata": {},
   "source": [
    "## Transformer Configuration Class  \n",
    "This class defines the hyperparameters for the GPT-2 model.  \n",
    "- `vocab_size`: Number of unique tokens in the dataset.  \n",
    "- `block_size`: Maximum context length for predictions.  \n",
    "- `n_embed`: Size of token embeddings.  \n",
    "- `n_head`: Number of attention heads.  \n",
    "- `n_layer`: Number of transformer layers.  \n",
    "- `dropout`: Dropout rate to prevent overfitting.  \n",
    "- `learning_rate`, `weight_decay`, `beta1`, `beta2`: Optimizer settings.  \n",
    "- `grad_clip`: Maximum gradient norm for clipping.  \n",
    "- `warmup_steps`, `total_steps`: Training schedule settings.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba313c",
   "metadata": {
    "id": "aa3e1188"
   },
   "outputs": [],
   "source": [
    "# Define the Transformer configuration class with model hyperparameters\n",
    "class TransformerConfig:\n",
    "    \"\"\"Configuration for the transformer model.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        block_size: int = 256,\n",
    "        n_embed: int = 384,\n",
    "        n_head: int = 6,\n",
    "        n_layer: int = 6,\n",
    "        dropout: float = 0.2,\n",
    "        learning_rate: float = 3e-4,\n",
    "        weight_decay: float = 0.1,\n",
    "        beta1: float = 0.9,\n",
    "        beta2: float = 0.95,\n",
    "        grad_clip: float = 1.0,\n",
    "        warmup_steps: int = 2000,\n",
    "        total_steps: int = 250000,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.grad_clip = grad_clip\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3337640",
   "metadata": {
    "id": "ggUDGuSqR7FJ"
   },
   "outputs": [],
   "source": [
    "class CustomTrainState(train_state.TrainState):\n",
    "    \"\"\"Custom train state with additional fields if needed.\"\"\"\n",
    "    # Add any additional fields here if necessary\n",
    "    pass\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention with optimized implementation.\"\"\"\n",
    "    config: TransformerConfig\n",
    "\n",
    "    def setup(self):\n",
    "        config = self.config\n",
    "        assert config.n_embed % config.n_head == 0\n",
    "\n",
    "        # Key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Dense(\n",
    "            3 * config.n_embed,\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        self.c_proj = nn.Dense(\n",
    "            config.n_embed,\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "\n",
    "        # Causal mask to ensure attention only to previous tokens\n",
    "        # Instead of register_buffer, we'll make it a class variable\n",
    "        self.bias = jnp.tril(jnp.ones((config.block_size, config.block_size)))\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        self.head_dim = config.n_embed // config.n_head\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        B, T, C = x.shape  # batch size, sequence length, embedding dim\n",
    "\n",
    "        # Calculate query, key, values for all heads in batch\n",
    "        q, k, v = jnp.split(self.c_attn(x), 3, axis=-1)\n",
    "\n",
    "        # Reshape to (B, T, nh, hs)\n",
    "        k = k.reshape(B, T, self.n_head, self.head_dim)\n",
    "        q = q.reshape(B, T, self.n_head, self.head_dim)\n",
    "        v = v.reshape(B, T, self.n_head, self.head_dim)\n",
    "\n",
    "        # Transpose to (B, nh, T, hs)\n",
    "        k = jnp.transpose(k, (0, 2, 1, 3))\n",
    "        q = jnp.transpose(q, (0, 2, 1, 3))\n",
    "        v = jnp.transpose(v, (0, 2, 1, 3))\n",
    "\n",
    "        # Efficient scaled dot-product attention\n",
    "        scale = jnp.sqrt(self.head_dim)\n",
    "        att = (q @ jnp.transpose(k, (0, 1, 3, 2))) / scale  # (B, nh, T, T)\n",
    "\n",
    "        # Causal mask to ensure attention only to past tokens\n",
    "        mask = self.bias[:T, :T]\n",
    "        # Use jnp.where with a mask for better TPU compatibility\n",
    "        att = jnp.where(mask == 0, jnp.full_like(att, -1e10), att)  # (B, nh, T, T)\n",
    "\n",
    "        # Softmax attention\n",
    "        att = jax.nn.softmax(att, axis=-1)\n",
    "        att = self.attn_dropout(att, deterministic=not training)\n",
    "\n",
    "        # Combine heads\n",
    "        y = att @ v  # (B, nh, T, hs)\n",
    "        y = jnp.transpose(y, (0, 2, 1, 3))  # (B, T, nh, hs)\n",
    "        y = y.reshape(B, T, C)  # (B, T, C)\n",
    "\n",
    "        # Output projection\n",
    "        y = self.resid_dropout(self.c_proj(y), deterministic=not training)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP with better initialization and gelu activation.\"\"\"\n",
    "    config: TransformerConfig\n",
    "\n",
    "    def setup(self):\n",
    "        config = self.config\n",
    "        self.c_fc = nn.Dense(\n",
    "            4 * config.n_embed,\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        # Use approximate=True for faster GELU on TPUs\n",
    "        self.gelu = lambda x: jax.nn.gelu(x, approximate=True)\n",
    "        self.c_proj = nn.Dense(\n",
    "            config.n_embed,\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x, deterministic=not training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block with pre-layer normalization.\"\"\"\n",
    "    config: TransformerConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln_1 = nn.LayerNorm(epsilon=1e-5)\n",
    "        self.attn = CausalSelfAttention(self.config)\n",
    "        self.ln_2 = nn.LayerNorm(epsilon=1e-5)\n",
    "        self.mlp = MLP(self.config)\n",
    "\n",
    "    def __call__(self, x, training=False):\n",
    "        # Pre-layer normalization design\n",
    "        x = x + self.attn(self.ln_1(x), training=training)\n",
    "        x = x + self.mlp(self.ln_2(x), training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT Language Model with improved implementation.\"\"\"\n",
    "    config: TransformerConfig\n",
    "\n",
    "    def setup(self):\n",
    "        config = self.config\n",
    "\n",
    "        # Token and position embeddings\n",
    "        self.wte = nn.Embed(\n",
    "            config.vocab_size,\n",
    "            config.n_embed,\n",
    "            embedding_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "        self.wpe = nn.Embed(\n",
    "            config.block_size,\n",
    "            config.n_embed,\n",
    "            embedding_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = [Block(config) for _ in range(config.n_layer)]\n",
    "\n",
    "        # Final layer norm and head\n",
    "        self.ln_f = nn.LayerNorm(epsilon=1e-5)\n",
    "        self.lm_head = nn.Dense(\n",
    "            config.vocab_size,\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros,\n",
    "            use_bias=False\n",
    "        )\n",
    "\n",
    "        # For weight tying\n",
    "        self.apply_weight_tying = True\n",
    "\n",
    "    def _tie_weights(self, params):\n",
    "        \"\"\"Tie embedding weights with output layer if enabled.\"\"\"\n",
    "        if not self.apply_weight_tying:\n",
    "            return params\n",
    "\n",
    "        # Clone the parameters and update lm_head kernel with embedding weights\n",
    "        new_params = params.copy()\n",
    "        new_params['lm_head']['kernel'] = new_params['wte']['embedding']\n",
    "        return new_params\n",
    "\n",
    "    def __call__(self, idx, targets=None, training=False, params=None):\n",
    "        config = self.config\n",
    "        b, t = idx.shape\n",
    "\n",
    "        # Apply weight tying if enabled (only during inference)\n",
    "        if params is not None and self.apply_weight_tying and not training:\n",
    "            params = self._tie_weights(params)\n",
    "\n",
    "        # Get token and position embeddings\n",
    "        token_emb = self.wte(idx)  # (b, t, n_embed)\n",
    "        pos = jnp.arange(0, t, dtype=jnp.int32)\n",
    "        pos_emb = self.wpe(pos)  # (t, n_embed)\n",
    "\n",
    "        # Sum embeddings and apply dropout\n",
    "        x = token_emb + pos_emb\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, training=training)\n",
    "\n",
    "        # Apply final layer norm\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Project to vocabulary\n",
    "        logits = self.lm_head(x)  # (b, t, vocab_size)\n",
    "\n",
    "        # If targets are provided, compute loss\n",
    "        if targets is not None:\n",
    "            # Cross-entropy loss for next token prediction\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "                logits.reshape(-1, config.vocab_size),\n",
    "                targets.reshape(-1)\n",
    "            ).mean()\n",
    "            return logits, loss\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1ed8e",
   "metadata": {
    "id": "onvhoeOfSAjX"
   },
   "outputs": [],
   "source": [
    "# ---------- Data Loading ----------\n",
    "def get_batch(rng_key, data, batch_size, block_size):\n",
    "    \"\"\"Get a random batch of data with safety checks.\"\"\"\n",
    "    data_size = data.shape[0]\n",
    "\n",
    "    # Safety check: ensure data is large enough\n",
    "    if data_size <= block_size:\n",
    "        raise ValueError(f\"Data size ({data_size}) must be larger than block size ({block_size})\")\n",
    "\n",
    "    rng_key, split_key = jax.random.split(rng_key)\n",
    "\n",
    "    # Use jax.random.randint for generating random indices\n",
    "    max_start_idx = data_size - block_size - 1\n",
    "    indices = jax.random.randint(\n",
    "        split_key,\n",
    "        shape=(batch_size,),\n",
    "        minval=0,\n",
    "        maxval=max_start_idx\n",
    "    )\n",
    "\n",
    "    # TPU-optimized data loading using vectorized operations\n",
    "    idx = jnp.arange(block_size)\n",
    "    offsets = indices[:, None]\n",
    "    x_indices = offsets + idx\n",
    "    y_indices = offsets + idx + 1\n",
    "\n",
    "    x = jnp.take(data, x_indices, axis=0)\n",
    "    y = jnp.take(data, y_indices, axis=0)\n",
    "\n",
    "    return x, y, rng_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127aee6",
   "metadata": {
    "id": "3uCdC307SGXG"
   },
   "outputs": [],
   "source": [
    "def create_train_state(rng_key, config):\n",
    "    \"\"\"Create initial training state.\"\"\"\n",
    "    model = GPT(config)\n",
    "\n",
    "    # Initialize model parameters with a properly shaped input\n",
    "    dummy_input = jnp.ones((8, 64), dtype=jnp.int32)  # Use batch size divisible by 8 for TPU\n",
    "    params = model.init(rng_key, dummy_input, training=False)[\"params\"]\n",
    "\n",
    "    # Create optimizer with learning rate schedule\n",
    "    lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=config.learning_rate,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        decay_steps=config.total_steps,\n",
    "        end_value=config.learning_rate * 0.1,\n",
    "    )\n",
    "\n",
    "    # AdamW optimizer with weight decay and gradient clipping\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip_by_global_norm(config.grad_clip),\n",
    "        optax.adamw(\n",
    "            learning_rate=lr_schedule,\n",
    "            b1=config.beta1,\n",
    "            b2=config.beta2,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return CustomTrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer\n",
    "    )\n",
    "\n",
    "# TPU-optimized training step with pmap support\n",
    "@partial(jax.pmap, axis_name='batch', static_broadcasted_argnums=(3,))\n",
    "def train_step_pmap(state, batch, rng_keys, training=True):\n",
    "    \"\"\"Single training step with parallel processing support for TPUs.\"\"\"\n",
    "    inputs, targets = batch\n",
    "\n",
    "    # Use different dropout key for each step and device\n",
    "    dropout_rng = jax.random.fold_in(\n",
    "        rng_keys,\n",
    "        state.step\n",
    "    )\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits, loss = state.apply_fn(\n",
    "            {\"params\": params},\n",
    "            inputs,\n",
    "            targets=targets,\n",
    "            training=training,\n",
    "            rngs={\"dropout\": dropout_rng}\n",
    "        )\n",
    "        return loss, logits\n",
    "\n",
    "    # Compute loss and gradients\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "\n",
    "    # Average gradients across replicas\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
    "\n",
    "    # Update model parameters\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"perplexity\": jnp.exp(loss),\n",
    "    }\n",
    "\n",
    "    return new_state, metrics, logits\n",
    "\n",
    "\n",
    "# Standard training step for single device\n",
    "@partial(jax.jit, static_argnums=(3,))\n",
    "def train_step(state, batch, rng_key, training=True):\n",
    "    \"\"\"Single training step for single device.\"\"\"\n",
    "    inputs, targets = batch\n",
    "\n",
    "    # Use different dropout key for each step\n",
    "    dropout_rng = jax.random.fold_in(rng_key, state.step)\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits, loss = state.apply_fn(\n",
    "            {\"params\": params},\n",
    "            inputs,\n",
    "            targets=targets,\n",
    "            training=training,\n",
    "            rngs={\"dropout\": dropout_rng}\n",
    "        )\n",
    "        return loss, logits\n",
    "\n",
    "    # Compute loss and gradients\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(state.params)\n",
    "\n",
    "    # Update model parameters\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"perplexity\": jnp.exp(loss),\n",
    "    }\n",
    "\n",
    "    return new_state, metrics, logits\n",
    "\n",
    "\n",
    "# TPU-optimized evaluation step with pmap support\n",
    "@partial(jax.pmap, axis_name='batch', static_broadcasted_argnums=(2,))\n",
    "def eval_step_pmap(state, batch, training=False):\n",
    "    \"\"\"Evaluation step with parallel processing support for TPUs.\"\"\"\n",
    "    inputs, targets = batch\n",
    "    logits, loss = state.apply_fn(\n",
    "        {\"params\": state.params},\n",
    "        inputs,\n",
    "        targets=targets,\n",
    "        training=training,\n",
    "    )\n",
    "\n",
    "    # Average loss across replicas\n",
    "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"perplexity\": jnp.exp(loss),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Standard evaluation step for single device\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def eval_step(state, batch, training=False):\n",
    "    \"\"\"Evaluation step for single device.\"\"\"\n",
    "    inputs, targets = batch\n",
    "    logits, loss = state.apply_fn(\n",
    "        {\"params\": state.params},\n",
    "        inputs,\n",
    "        targets=targets,\n",
    "        training=training,\n",
    "    )\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"perplexity\": jnp.exp(loss),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae28ea3b",
   "metadata": {
    "id": "up4L7GuoSeXs"
   },
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(2, 3, 4, 5))\n",
    "def generate_step(params, idx, block_size, temperature=1.0, top_k=40, apply_fn=None, rng_key=None):\n",
    "    \"\"\"Single generation step using top-k sampling.\"\"\"\n",
    "    # Take the last block_size tokens as context (or fewer if not enough)\n",
    "    context_size = min(idx.shape[1], block_size)\n",
    "    idx_cond = idx[:, -context_size:]\n",
    "\n",
    "    # Get logits from the model\n",
    "    logits = apply_fn({\"params\": params}, idx_cond, training=False)\n",
    "\n",
    "    # Focus only on the last time step\n",
    "    logits = logits[:, -1, :] / temperature\n",
    "\n",
    "    # Optional top-k sampling using JAX's efficient operators\n",
    "    if top_k > 0:\n",
    "        top_k = min(top_k, logits.shape[-1])\n",
    "        topk_values, _ = jax.lax.top_k(logits, top_k)\n",
    "        threshold = topk_values[:, -1]\n",
    "        logits = jnp.where(logits < threshold[:, None], jnp.full_like(logits, -1e10), logits)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    sample = jax.random.categorical(rng_key, logits, axis=-1)\n",
    "\n",
    "    # Append to the sequence\n",
    "    return jnp.concatenate([idx, sample[:, None]], axis=1)\n",
    "\n",
    "\n",
    "def generate(\n",
    "    params,\n",
    "    apply_fn,\n",
    "    prompt_idx,\n",
    "    rng_key,\n",
    "    max_new_tokens=100,\n",
    "    temperature=1.0,\n",
    "    top_k=40,\n",
    "    block_size=256\n",
    "):\n",
    "    \"\"\"Generate text using the model with proper handling of long prompts.\"\"\"\n",
    "    # Handle prompt that may be longer than block_size\n",
    "    if prompt_idx.shape[1] > block_size:\n",
    "        # Only keep the last block_size tokens of the prompt\n",
    "        idx = prompt_idx[:, -block_size:]\n",
    "        print(f\"Warning: Prompt was truncated to the last {block_size} tokens due to context length limit.\")\n",
    "    else:\n",
    "        idx = prompt_idx\n",
    "\n",
    "    # Generate tokens one by one\n",
    "    for _ in range(max_new_tokens):\n",
    "        rng_key, next_key = jax.random.split(rng_key)\n",
    "        idx = generate_step(\n",
    "            params,\n",
    "            idx,\n",
    "            block_size,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            apply_fn=apply_fn,\n",
    "            rng_key=next_key\n",
    "        )\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "# ---------- TPU Initialization and Multi-device Training ----------\n",
    "def initialize_tpu():\n",
    "    \"\"\"Initialize TPU system.\"\"\"\n",
    "    # Check if running on TPU\n",
    "    if 'tpu' in jax.devices()[0].platform:\n",
    "        print(f\"Running on {jax.device_count()} TPU devices\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Not running on TPU\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def replicate_state_on_devices(state):\n",
    "    \"\"\"Replicate state across all TPU devices.\"\"\"\n",
    "    # Broadcast the state to all devices\n",
    "    state = jax.device_put_replicated(state, jax.local_devices())\n",
    "    return state\n",
    "\n",
    "\n",
    "def prepare_batch_for_devices(batch, num_devices):\n",
    "    \"\"\"Prepare batch for multiple devices by reshaping.\"\"\"\n",
    "    x, y = batch\n",
    "\n",
    "    # Get total batch size and calculate per-device batch size\n",
    "    total_batch_size = x.shape[0]\n",
    "    per_device_batch_size = total_batch_size // num_devices\n",
    "\n",
    "    # Reshape to (num_devices, per_device_batch_size, ...)\n",
    "    x = x.reshape((num_devices, per_device_batch_size) + x.shape[1:])\n",
    "    y = y.reshape((num_devices, per_device_batch_size) + y.shape[1:])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_device_rng_keys(rng_key, num_devices):\n",
    "    \"\"\"Create separate RNG keys for each device.\"\"\"\n",
    "    # Split the main key into num_devices keys\n",
    "    return jax.random.split(rng_key, num_devices)\n",
    "\n",
    "\n",
    "def step_device_rng_keys(rng_keys):\n",
    "    \"\"\"Update RNG keys for each device independently.\"\"\"\n",
    "    # Split each device's key to get a new key for each device\n",
    "    new_keys = jax.vmap(jax.random.split)(rng_keys)\n",
    "    # Each split returns 2 keys, keep the first one for each device\n",
    "    return new_keys[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7178f15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "vXDYFJ93Shje",
    "outputId": "5ffbaf17-81b3-45f7-a635-3f5cc6e51dec"
   },
   "outputs": [],
   "source": [
    "use_tpu = initialize_tpu()\n",
    "num_devices = jax.device_count()\n",
    "\n",
    "# 2. Load and prepare data with proper error handling\n",
    "input_file = 'input.txt'\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"Error: Input file '{input_file}' not found.\")\n",
    "\n",
    "try:\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Safety check: ensure we have enough data\n",
    "    if len(text) < 1000:\n",
    "        print(\"Warning: Text data is very short. Training may not be effective.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading input file: {e}\")\n",
    "\n",
    "# Get unique characters and create mappings\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# Encode the entire text\n",
    "data = jnp.array(encode(text))\n",
    "\n",
    "# Split into train and validation sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# 3. Create configuration with adaptive total_steps\n",
    "# Calculate reasonable total_steps based on dataset size\n",
    "data_size = len(train_data)\n",
    "batch_size = min(64, data_size // 10)  # Cap batch size at 64\n",
    "if use_tpu:\n",
    "    # Scale batch size responsibly\n",
    "    devices_to_use = min(num_devices, 8)  # Cap device usage if too many\n",
    "    batch_size = batch_size * devices_to_use\n",
    "\n",
    "# Estimate total steps based on dataset size and batch size\n",
    "# Aim for ~100 epochs for small datasets, fewer for larger ones\n",
    "epochs = max(10, min(100, 1000000 // data_size))\n",
    "total_steps = (data_size * epochs) // batch_size\n",
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=min(256, data_size // 2),  # Ensure block_size isn't too large\n",
    "    n_embed=384,\n",
    "    n_head=6,\n",
    "    n_layer=6,\n",
    "    dropout=0.2,\n",
    "    learning_rate=3e-4,\n",
    "    total_steps=total_steps,\n",
    ")\n",
    "\n",
    "# 4. Initialize model and training state\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "rng_key, init_key = jax.random.split(rng_key)\n",
    "train_state = create_train_state(init_key, config)\n",
    "\n",
    "# For TPU, replicate the state across devices\n",
    "if use_tpu:\n",
    "    train_state = replicate_state_on_devices(train_state)\n",
    "    # Create separate rng keys for each device\n",
    "    device_rng_keys = create_device_rng_keys(rng_key, num_devices)\n",
    "\n",
    "# 5. Training loop\n",
    "print(f\"Training for {total_steps} steps with batch size {batch_size}\")\n",
    "steps_to_run = min(5000, total_steps)  # Cap at 5000 steps for example\n",
    "\n",
    "for step in range(steps_to_run):\n",
    "    try:\n",
    "        # Get batch\n",
    "        if use_tpu:\n",
    "            # For TPU, create a batch for each device\n",
    "            x, y, rng_key = get_batch(rng_key, train_data, batch_size, config.block_size)\n",
    "            x, y = prepare_batch_for_devices((x, y), num_devices)\n",
    "\n",
    "            # Update device RNG keys correctly\n",
    "            device_rng_keys = step_device_rng_keys(device_rng_keys)\n",
    "\n",
    "            # Train step with parallel map\n",
    "            train_state, metrics, _ = train_step_pmap(train_state, (x, y), device_rng_keys, True)\n",
    "\n",
    "            # Extract metrics from first device (all are same due to pmean)\n",
    "            metrics = {k: v[0] for k, v in metrics.items()}\n",
    "        else:\n",
    "            # Standard single-device training\n",
    "            x, y, rng_key = get_batch(rng_key, train_data, batch_size, config.block_size)\n",
    "            train_state, metrics, _ = train_step(train_state, (x, y), rng_key)\n",
    "\n",
    "        # Print metrics occasionally\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}: loss = {metrics['loss']:.4f}, perplexity = {metrics['perplexity']:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training step {step}: {e}\")\n",
    "        break\n",
    "\n",
    "# 6. Generate text (on a single device)\n",
    "try:\n",
    "    prompt = \"ROMEO:\"\n",
    "    print(f\"Generating text from prompt: '{prompt}'\")\n",
    "    prompt_idx = jnp.array([encode(prompt)])\n",
    "\n",
    "    # If using TPU, get params from first device\n",
    "    if use_tpu:\n",
    "        params = jax.tree_map(lambda x: x[0], train_state.params)\n",
    "        apply_fn = train_state.apply_fn\n",
    "    else:\n",
    "        params = train_state.params\n",
    "        apply_fn = train_state.apply_fn\n",
    "\n",
    "    rng_key, gen_key = jax.random.split(rng_key)\n",
    "    generated_idx = generate(\n",
    "        params,\n",
    "        apply_fn,\n",
    "        prompt_idx,\n",
    "        gen_key,\n",
    "        max_new_tokens=500,\n",
    "        temperature=0.8,\n",
    "        top_k=40,\n",
    "        block_size=config.block_size\n",
    "    )\n",
    "\n",
    "    generated_text = decode(generated_idx[0].tolist())\n",
    "    print(generated_text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during text generation: {e}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
