{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [],
      "dockerImageVersionId": 30920,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "8b39517e",
      "cell_type": "markdown",
      "source": [
        "# GPT-2 JAX Implementation\n",
        "\n",
        "This notebook demonstrates how to implement and use GPT-2 with JAX for efficient text generation. It includes model loading, inference, and text generation using JAX-based tensor computations. The notebook is optimized for TPU usage but can also run on CPUs or GPUs."
      ],
      "metadata": {
        "id": "8b39517e"
      }
    },
    {
      "id": "80b92b23",
      "cell_type": "markdown",
      "source": [
        "## Importing Required Libraries"
      ],
      "metadata": {
        "id": "80b92b23"
      }
    },
    {
      "id": "631acac9",
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tqdm datasets"
      ],
      "metadata": {
        "id": "631acac9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:18:59.226143Z",
          "iopub.execute_input": "2025-03-19T14:18:59.226506Z",
          "iopub.status.idle": "2025-03-19T14:19:02.890815Z",
          "shell.execute_reply.started": "2025-03-19T14:18:59.226478Z",
          "shell.execute_reply": "2025-03-19T14:19:02.889252Z"
        }
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "8qohV72gSRbO",
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "from typing import Any, Callable, Dict, Optional, Tuple\n",
        "from flax.training import train_state\n",
        "from functools import partial\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8qohV72gSRbO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.899829Z",
          "iopub.execute_input": "2025-03-19T14:19:02.900039Z",
          "iopub.status.idle": "2025-03-19T14:19:02.915917Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.900019Z",
          "shell.execute_reply": "2025-03-19T14:19:02.914552Z"
        }
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "fLe9SKFe-Ylg",
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "We will use the `tiny_shakespeare` dataset for this example. Other datasets like `wikitext2`, `bookcorpus`, and `oscar` can also be used but may take longer to process.`\n",
        "\n"
      ],
      "metadata": {
        "id": "fLe9SKFe-Ylg"
      }
    },
    {
      "id": "uhsRqRpd-VMa",
      "cell_type": "code",
      "source": [
        "dataset_name = \"karpathy/tiny_shakespeare\"\n",
        "\"\"\"\n",
        "Other examples may include :\n",
        "[Note that these may take longer]\n",
        "\n",
        "dataset_name = \"mindchain/wikitext2\"  # WikiText dataset\n",
        "dataset_name = \"SamuelYang/bookcorpus\"  # BookCorpus dataset\n",
        "dataset_name = \"oscar-corpus/oscar\"  # OSCAR dataset\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uhsRqRpd-VMa",
        "outputId": "22cd03f9-5739-41ff-9772-39639524d817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOther examples may include :\\n[Note that these may take longer]\\n\\ndataset_name = \"mindchain/wikitext2\"  # WikiText dataset\\ndataset_name = \"SamuelYang/bookcorpus\"  # BookCorpus dataset\\ndataset_name = \"oscar-corpus/oscar\"  # OSCAR dataset\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3
    },
    {
      "id": "cxINIgc2-dzx",
      "cell_type": "markdown",
      "source": [
        "## Model Configuration\n",
        "The `TransformerConfig` class defines the configuration for the GPT-2 model, including hyperparameters like vocabulary size, embedding size, number of layers, and learning rate.\n",
        "\n"
      ],
      "metadata": {
        "id": "cxINIgc2-dzx"
      }
    },
    {
      "id": "aa3e1188",
      "cell_type": "code",
      "source": [
        "class TransformerConfig:\n",
        "    \"\"\"Configuration for the transformer model.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        block_size: int = 256,\n",
        "        n_embed: int = 384,\n",
        "        n_head: int = 6,\n",
        "        n_layer: int = 6,\n",
        "        dropout: float = 0.2,\n",
        "        learning_rate: float = 3e-4,\n",
        "        weight_decay: float = 0.1,\n",
        "        beta1: float = 0.9,\n",
        "        beta2: float = 0.95,\n",
        "        grad_clip: float = 1.0,\n",
        "        warmup_steps: int = 2000,\n",
        "        total_steps: int = 250000,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_embed = n_embed\n",
        "        self.n_head = n_head\n",
        "        self.n_layer = n_layer\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.grad_clip = grad_clip\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = total_steps"
      ],
      "metadata": {
        "id": "aa3e1188",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.916737Z",
          "iopub.execute_input": "2025-03-19T14:19:02.916983Z",
          "iopub.status.idle": "2025-03-19T14:19:02.925698Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.916960Z",
          "shell.execute_reply": "2025-03-19T14:19:02.924693Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "_deGQQXf-k_3",
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "The GPT-2 model is implemented using JAX and Flax. The model consists of multiple transformer blocks, each containing multi-head self-attention and a feed-forward network.\n",
        "\n"
      ],
      "metadata": {
        "id": "_deGQQXf-k_3"
      }
    },
    {
      "id": "ggUDGuSqR7FJ",
      "cell_type": "code",
      "source": [
        "class CustomTrainState(train_state.TrainState):\n",
        "    \"\"\"Custom train state with additional fields if needed.\"\"\"\n",
        "    # Add any additional fields here if necessary\n",
        "    pass\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head causal self-attention with optimized implementation.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        config = self.config\n",
        "        assert config.n_embed % config.n_head == 0\n",
        "\n",
        "        # Key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Dense(\n",
        "            3 * config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Dense(\n",
        "            config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "\n",
        "        # Causal mask to ensure attention only to previous tokens\n",
        "        # Instead of register_buffer, we'll make it a class variable\n",
        "        self.bias = jnp.tril(jnp.ones((config.block_size, config.block_size)))\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed = config.n_embed\n",
        "        self.head_dim = config.n_embed // config.n_head\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        B, T, C = x.shape  # batch size, sequence length, embedding dim\n",
        "\n",
        "        # Calculate query, key, values for all heads in batch\n",
        "        q, k, v = jnp.split(self.c_attn(x), 3, axis=-1)\n",
        "\n",
        "        # Reshape to (B, T, nh, hs)\n",
        "        k = k.reshape(B, T, self.n_head, self.head_dim)\n",
        "        q = q.reshape(B, T, self.n_head, self.head_dim)\n",
        "        v = v.reshape(B, T, self.n_head, self.head_dim)\n",
        "\n",
        "        # Transpose to (B, nh, T, hs)\n",
        "        k = jnp.transpose(k, (0, 2, 1, 3))\n",
        "        q = jnp.transpose(q, (0, 2, 1, 3))\n",
        "        v = jnp.transpose(v, (0, 2, 1, 3))\n",
        "\n",
        "        # Efficient scaled dot-product attention\n",
        "        scale = jnp.sqrt(self.head_dim)\n",
        "        att = (q @ jnp.transpose(k, (0, 1, 3, 2))) / scale  # (B, nh, T, T)\n",
        "\n",
        "        # Causal mask to ensure attention only to past tokens\n",
        "        mask = self.bias[:T, :T]\n",
        "        # Use jnp.where with a mask for better TPU compatibility\n",
        "        att = jnp.where(mask == 0, jnp.full_like(att, -1e10), att)  # (B, nh, T, T)\n",
        "\n",
        "        # Softmax attention\n",
        "        att = jax.nn.softmax(att, axis=-1)\n",
        "        att = self.attn_dropout(att, deterministic=not training)\n",
        "\n",
        "        # Combine heads\n",
        "        y = att @ v  # (B, nh, T, hs)\n",
        "        y = jnp.transpose(y, (0, 2, 1, 3))  # (B, T, nh, hs)\n",
        "        y = y.reshape(B, T, C)  # (B, T, C)\n",
        "\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y), deterministic=not training)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"MLP with better initialization and gelu activation.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        config = self.config\n",
        "        self.c_fc = nn.Dense(\n",
        "            4 * config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "        # Use approximate=True for faster GELU on TPUs\n",
        "        self.gelu = lambda x: jax.nn.gelu(x, approximate=True)\n",
        "        self.c_proj = nn.Dense(\n",
        "            config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block with pre-layer normalization.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        self.ln_1 = nn.LayerNorm(epsilon=1e-5)\n",
        "        self.attn = CausalSelfAttention(self.config)\n",
        "        self.ln_2 = nn.LayerNorm(epsilon=1e-5)\n",
        "        self.mlp = MLP(self.config)\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        # Pre-layer normalization design\n",
        "        x = x + self.attn(self.ln_1(x), training=training)\n",
        "        x = x + self.mlp(self.ln_2(x), training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"GPT Language Model with improved implementation.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        config = self.config\n",
        "\n",
        "        # Token and position embeddings\n",
        "        self.wte = nn.Embed(\n",
        "            config.vocab_size,\n",
        "            config.n_embed,\n",
        "            embedding_init=nn.initializers.normal(stddev=0.02)\n",
        "        )\n",
        "        self.wpe = nn.Embed(\n",
        "            config.block_size,\n",
        "            config.n_embed,\n",
        "            embedding_init=nn.initializers.normal(stddev=0.02)\n",
        "        )\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = [Block(config) for _ in range(config.n_layer)]\n",
        "\n",
        "        # Final layer norm and head\n",
        "        self.ln_f = nn.LayerNorm(epsilon=1e-5)\n",
        "        self.lm_head = nn.Dense(\n",
        "            config.vocab_size,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros,\n",
        "            use_bias=False\n",
        "        )\n",
        "\n",
        "        # For weight tying\n",
        "        self.apply_weight_tying = True\n",
        "\n",
        "    def _tie_weights(self, params):\n",
        "        \"\"\"Tie embedding weights with output layer if enabled.\"\"\"\n",
        "        if not self.apply_weight_tying:\n",
        "            return params\n",
        "\n",
        "        # Clone the parameters and update lm_head kernel with embedding weights\n",
        "        new_params = params.copy()\n",
        "        new_params['lm_head']['kernel'] = new_params['wte']['embedding']\n",
        "        return new_params\n",
        "\n",
        "    def __call__(self, idx, targets=None, training=False, params=None):\n",
        "        config = self.config\n",
        "        b, t = idx.shape\n",
        "\n",
        "        # Apply weight tying if enabled (only during inference)\n",
        "        if params is not None and self.apply_weight_tying and not training:\n",
        "            params = self._tie_weights(params)\n",
        "\n",
        "        # Get token and position embeddings\n",
        "        token_emb = self.wte(idx)  # (b, t, n_embed)\n",
        "        pos = jnp.arange(0, t, dtype=jnp.int32)\n",
        "        pos_emb = self.wpe(pos)  # (t, n_embed)\n",
        "\n",
        "        # Sum embeddings and apply dropout\n",
        "        x = token_emb + pos_emb\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        # Apply final layer norm\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        logits = self.lm_head(x)  # (b, t, vocab_size)\n",
        "\n",
        "        # If targets are provided, compute loss\n",
        "        if targets is not None:\n",
        "            # Cross-entropy loss for next token prediction\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "                logits.reshape(-1, config.vocab_size),\n",
        "                targets.reshape(-1)\n",
        "            ).mean()\n",
        "            return logits, loss\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ggUDGuSqR7FJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.927068Z",
          "iopub.execute_input": "2025-03-19T14:19:02.927284Z",
          "iopub.status.idle": "2025-03-19T14:19:02.954053Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.927264Z",
          "shell.execute_reply": "2025-03-19T14:19:02.952934Z"
        }
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "onvhoeOfSAjX",
      "cell_type": "code",
      "source": [
        "# ---------- Data Loading ----------\n",
        "def get_batch(rng_key, data, batch_size, block_size):\n",
        "    \"\"\"Get a random batch of data with safety checks.\"\"\"\n",
        "    data_size = data.shape[0]\n",
        "\n",
        "    # Safety check: ensure data is large enough\n",
        "    if data_size <= block_size:\n",
        "        raise ValueError(f\"Data size ({data_size}) must be larger than block size ({block_size})\")\n",
        "\n",
        "    rng_key, split_key = jax.random.split(rng_key)\n",
        "\n",
        "    # Use jax.random.randint for generating random indices\n",
        "    max_start_idx = data_size - block_size - 1\n",
        "    indices = jax.random.randint(\n",
        "        split_key,\n",
        "        shape=(batch_size,),\n",
        "        minval=0,\n",
        "        maxval=max_start_idx\n",
        "    )\n",
        "\n",
        "    # TPU-optimized data loading using vectorized operations\n",
        "    idx = jnp.arange(block_size)\n",
        "    offsets = indices[:, None]\n",
        "    x_indices = offsets + idx\n",
        "    y_indices = offsets + idx + 1\n",
        "\n",
        "    x = jnp.take(data, x_indices, axis=0)\n",
        "    y = jnp.take(data, y_indices, axis=0)\n",
        "\n",
        "    return x, y, rng_key\n"
      ],
      "metadata": {
        "id": "onvhoeOfSAjX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.955045Z",
          "iopub.execute_input": "2025-03-19T14:19:02.955266Z",
          "iopub.status.idle": "2025-03-19T14:19:02.967596Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.955244Z",
          "shell.execute_reply": "2025-03-19T14:19:02.966614Z"
        }
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "w9ifLnBc-wmJ",
      "cell_type": "markdown",
      "source": [
        "## Training Setup\n",
        "The training setup includes functions for creating the training state, preparing batches, and defining the training and evaluation steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "w9ifLnBc-wmJ"
      }
    },
    {
      "id": "3uCdC307SGXG",
      "cell_type": "code",
      "source": [
        "def create_train_state(rng_key, config):\n",
        "    \"\"\"Create initial training state.\"\"\"\n",
        "    model = GPT(config)\n",
        "\n",
        "    # Initialize model parameters with a properly shaped input\n",
        "    dummy_input = jnp.ones((8, 64), dtype=jnp.int32)  # Use batch size divisible by 8 for TPU\n",
        "    params = model.init(rng_key, dummy_input, training=False)[\"params\"]\n",
        "\n",
        "    # Create optimizer with learning rate schedule\n",
        "    lr_schedule = optax.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=config.learning_rate,\n",
        "        warmup_steps=config.warmup_steps,\n",
        "        decay_steps=config.total_steps,\n",
        "        end_value=config.learning_rate * 0.1,\n",
        "    )\n",
        "\n",
        "    # AdamW optimizer with weight decay and gradient clipping\n",
        "    optimizer = optax.chain(\n",
        "        optax.clip_by_global_norm(config.grad_clip),\n",
        "        optax.adamw(\n",
        "            learning_rate=lr_schedule,\n",
        "            b1=config.beta1,\n",
        "            b2=config.beta2,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return CustomTrainState.create(\n",
        "        apply_fn=model.apply,\n",
        "        params=params,\n",
        "        tx=optimizer\n",
        "    )\n",
        "\n",
        "# TPU-optimized training step with pmap support\n",
        "@partial(jax.pmap, axis_name='batch', static_broadcasted_argnums=(3,))\n",
        "def train_step_pmap(state, batch, rng_keys, training=True):\n",
        "    \"\"\"Single training step with parallel processing support for TPUs.\"\"\"\n",
        "    inputs, targets = batch\n",
        "\n",
        "    # Use different dropout key for each step and device\n",
        "    dropout_rng = jax.random.fold_in(\n",
        "        rng_keys,\n",
        "        state.step\n",
        "    )\n",
        "\n",
        "    def loss_fn(params):\n",
        "        logits, loss = state.apply_fn(\n",
        "            {\"params\": params},\n",
        "            inputs,\n",
        "            targets=targets,\n",
        "            training=training,\n",
        "            rngs={\"dropout\": dropout_rng}\n",
        "        )\n",
        "        return loss, logits\n",
        "\n",
        "    # Compute loss and gradients\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(state.params)\n",
        "\n",
        "    # Average gradients across replicas\n",
        "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
        "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
        "\n",
        "    # Update model parameters\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "\n",
        "    return new_state, metrics, logits\n",
        "\n",
        "\n",
        "# Standard training step for single device\n",
        "@partial(jax.jit, static_argnums=(3,))\n",
        "def train_step(state, batch, rng_key, training=True):\n",
        "    \"\"\"Single training step for single device.\"\"\"\n",
        "    inputs, targets = batch\n",
        "\n",
        "    # Use different dropout key for each step\n",
        "    dropout_rng = jax.random.fold_in(rng_key, state.step)\n",
        "\n",
        "    def loss_fn(params):\n",
        "        logits, loss = state.apply_fn(\n",
        "            {\"params\": params},\n",
        "            inputs,\n",
        "            targets=targets,\n",
        "            training=training,\n",
        "            rngs={\"dropout\": dropout_rng}\n",
        "        )\n",
        "        return loss, logits\n",
        "\n",
        "    # Compute loss and gradients\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(state.params)\n",
        "\n",
        "    # Update model parameters\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "\n",
        "    return new_state, metrics, logits\n",
        "\n",
        "\n",
        "# TPU-optimized evaluation step with pmap support\n",
        "@partial(jax.pmap, axis_name='batch', static_broadcasted_argnums=(2,))\n",
        "def eval_step_pmap(state, batch, training=False):\n",
        "    \"\"\"Evaluation step with parallel processing support for TPUs.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    logits, loss = state.apply_fn(\n",
        "        {\"params\": state.params},\n",
        "        inputs,\n",
        "        targets=targets,\n",
        "        training=training,\n",
        "    )\n",
        "\n",
        "    # Average loss across replicas\n",
        "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Standard evaluation step for single device\n",
        "@partial(jax.jit, static_argnums=(2,))\n",
        "def eval_step(state, batch, training=False):\n",
        "    \"\"\"Evaluation step for single device.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    logits, loss = state.apply_fn(\n",
        "        {\"params\": state.params},\n",
        "        inputs,\n",
        "        targets=targets,\n",
        "        training=training,\n",
        "    )\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "3uCdC307SGXG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.968679Z",
          "iopub.execute_input": "2025-03-19T14:19:02.968924Z",
          "iopub.status.idle": "2025-03-19T14:19:02.984855Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.968903Z",
          "shell.execute_reply": "2025-03-19T14:19:02.983825Z"
        }
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "id": "up4L7GuoSeXs",
      "cell_type": "code",
      "source": [
        "@partial(jax.jit, static_argnums=(2, 3, 4, 5))\n",
        "def generate_step(params, idx, block_size, temperature=1.0, top_k=40, apply_fn=None, rng_key=None):\n",
        "    \"\"\"Single generation step using top-k sampling.\"\"\"\n",
        "    # Take the last block_size tokens as context (or fewer if not enough)\n",
        "    context_size = min(idx.shape[1], block_size)\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "\n",
        "    # Get logits from the model\n",
        "    logits = apply_fn({\"params\": params}, idx_cond, training=False)\n",
        "\n",
        "    # Focus only on the last time step\n",
        "    logits = logits[:, -1, :] / temperature\n",
        "\n",
        "    # Optional top-k sampling using JAX's efficient operators\n",
        "    if top_k > 0:\n",
        "        top_k = min(top_k, logits.shape[-1])\n",
        "        topk_values, _ = jax.lax.top_k(logits, top_k)\n",
        "        threshold = topk_values[:, -1]\n",
        "        logits = jnp.where(logits < threshold[:, None], jnp.full_like(logits, -1e10), logits)\n",
        "\n",
        "    # Sample from the distribution\n",
        "    sample = jax.random.categorical(rng_key, logits, axis=-1)\n",
        "\n",
        "    # Append to the sequence\n",
        "    return jnp.concatenate([idx, sample[:, None]], axis=1)\n",
        "\n",
        "\n",
        "def generate(\n",
        "    params,\n",
        "    apply_fn,\n",
        "    prompt_idx,\n",
        "    rng_key,\n",
        "    max_new_tokens=100,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    block_size=256\n",
        "):\n",
        "    \"\"\"Generate text using the model with proper handling of long prompts.\"\"\"\n",
        "    # Handle prompt that may be longer than block_size\n",
        "    if prompt_idx.shape[1] > block_size:\n",
        "        # Only keep the last block_size tokens of the prompt\n",
        "        idx = prompt_idx[:, -block_size:]\n",
        "        print(f\"Warning: Prompt was truncated to the last {block_size} tokens due to context length limit.\")\n",
        "    else:\n",
        "        idx = prompt_idx\n",
        "\n",
        "    # Generate tokens one by one\n",
        "    for _ in range(max_new_tokens):\n",
        "        rng_key, next_key = jax.random.split(rng_key)\n",
        "        idx = generate_step(\n",
        "            params,\n",
        "            idx,\n",
        "            block_size,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            apply_fn=apply_fn,\n",
        "            rng_key=next_key\n",
        "        )\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "# ---------- TPU Initialization and Multi-device Training ----------\n",
        "def initialize_tpu():\n",
        "    \"\"\"Initialize TPU system.\"\"\"\n",
        "    # Check if running on TPU\n",
        "    if 'tpu' in jax.devices()[0].platform:\n",
        "        print(f\"Running on {jax.device_count()} TPU devices\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Not running on TPU\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def replicate_state_on_devices(state):\n",
        "    \"\"\"Replicate state across all TPU devices.\"\"\"\n",
        "    # Broadcast the state to all devices\n",
        "    state = jax.device_put_replicated(state, jax.local_devices())\n",
        "    return state\n",
        "\n",
        "\n",
        "def prepare_batch_for_devices(batch, num_devices):\n",
        "    \"\"\"Prepare batch for multiple devices by reshaping.\"\"\"\n",
        "    x, y = batch\n",
        "\n",
        "    # Get total batch size and calculate per-device batch size\n",
        "    total_batch_size = x.shape[0]\n",
        "    per_device_batch_size = total_batch_size // num_devices\n",
        "\n",
        "    # Reshape to (num_devices, per_device_batch_size, ...)\n",
        "    x = x.reshape((num_devices, per_device_batch_size) + x.shape[1:])\n",
        "    y = y.reshape((num_devices, per_device_batch_size) + y.shape[1:])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def create_device_rng_keys(rng_key, num_devices):\n",
        "    \"\"\"Create separate RNG keys for each device.\"\"\"\n",
        "    # Split the main key into num_devices keys\n",
        "    return jax.random.split(rng_key, num_devices)\n",
        "\n",
        "\n",
        "def step_device_rng_keys(rng_keys):\n",
        "    \"\"\"Update RNG keys for each device independently.\"\"\"\n",
        "    # Split each device's key to get a new key for each device\n",
        "    new_keys = jax.vmap(jax.random.split)(rng_keys)\n",
        "    # Each split returns 2 keys, keep the first one for each device\n",
        "    return new_keys[:, 0]"
      ],
      "metadata": {
        "id": "up4L7GuoSeXs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.985847Z",
          "iopub.execute_input": "2025-03-19T14:19:02.986052Z",
          "iopub.status.idle": "2025-03-19T14:19:03.001007Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.986032Z",
          "shell.execute_reply": "2025-03-19T14:19:02.999819Z"
        }
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "mISHmd0eoap9",
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset_name, split=\"train\"):\n",
        "    \"\"\"\n",
        "    Prepare a Hugging Face dataset for training.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: str, name of the dataset on Hugging Face\n",
        "        split: str, which split to use (default: \"train\")\n",
        "\n",
        "    Returns:\n",
        "        train_data: encoded training data\n",
        "        val_data: encoded validation data\n",
        "        encode_fn: function to encode text\n",
        "        decode_fn: function to decode indices\n",
        "        vocab_size: size of vocabulary\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(dataset_name)\n",
        "\n",
        "    # Get text from dataset\n",
        "    if \"text\" in dataset[split].features:\n",
        "        text_key = \"text\"\n",
        "    else:\n",
        "        # Try to find a text field\n",
        "        text_fields = [k for k, v in dataset[split].features.items()\n",
        "                      if v.dtype == 'string']\n",
        "        if text_fields:\n",
        "            text_key = text_fields[0]\n",
        "        else:\n",
        "            raise ValueError(\"Could not find text field in dataset\")\n",
        "\n",
        "    # Combine all texts\n",
        "    text = \"\\n\".join(dataset[split][text_key])\n",
        "\n",
        "    # Create vocabulary\n",
        "    chars = sorted(list(set(text)))\n",
        "    vocab_size = len(chars)\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    # Create encode/decode functions\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "    # Encode full text\n",
        "    data = jnp.array(encode(text))\n",
        "\n",
        "    # Split into train/val\n",
        "    n = int(0.9 * len(data))\n",
        "    train_data = data[:n]\n",
        "    val_data = data[n:]\n",
        "\n",
        "    return train_data, val_data, encode, decode, chars, vocab_size"
      ],
      "metadata": {
        "id": "mISHmd0eoap9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:03.002125Z",
          "iopub.execute_input": "2025-03-19T14:19:03.002362Z",
          "iopub.status.idle": "2025-03-19T14:19:03.014291Z",
          "shell.execute_reply.started": "2025-03-19T14:19:03.002338Z",
          "shell.execute_reply": "2025-03-19T14:19:03.013316Z"
        }
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "id": "tuwPnU55-1_e",
      "cell_type": "markdown",
      "source": [
        "## Training Loop\n",
        "The training loop runs for a specified number of steps, printing the loss and perplexity periodically.\n",
        "\n"
      ],
      "metadata": {
        "id": "tuwPnU55-1_e"
      }
    },
    {
      "id": "vXDYFJ93Shje",
      "cell_type": "code",
      "source": [
        "use_tpu = initialize_tpu()\n",
        "num_devices = jax.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXDYFJ93Shje",
        "outputId": "23695ae8-4103-4593-945e-eed4a327c65e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:03.015333Z",
          "iopub.execute_input": "2025-03-19T14:19:03.015544Z",
          "iopub.status.idle": "2025-03-19T14:19:03.026333Z",
          "shell.execute_reply.started": "2025-03-19T14:19:03.015523Z",
          "shell.execute_reply": "2025-03-19T14:19:03.025286Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on 8 TPU devices\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "id": "0DxBfEE_c23p",
      "cell_type": "code",
      "source": [
        "# Load dataset from Hugging Face\n",
        "train_data, val_data, encode, decode, chars, vocab_size = prepare_dataset(dataset_name)\n",
        "# Calculate reasonable total_steps based on dataset size\n",
        "data_size = len(train_data)\n",
        "batch_size = min(64, data_size // 10)  # Cap batch size at 64\n",
        "if use_tpu:\n",
        "    # Scale batch size responsibly\n",
        "    devices_to_use = min(num_devices, 8)  # Cap device usage if too many\n",
        "    batch_size = batch_size * devices_to_use\n",
        "\n",
        "# Estimate total steps based on dataset size and batch size\n",
        "# Aim for ~100 epochs for small datasets, fewer for larger ones\n",
        "epochs = max(10, min(100, 1000000 // data_size))\n",
        "total_steps = (data_size * epochs) // batch_size\n",
        "\n",
        "config = TransformerConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=min(256, data_size // 2),  # Ensure block_size isn't too large\n",
        "    n_embed=384,\n",
        "    n_head=6,\n",
        "    n_layer=6,\n",
        "    dropout=0.2,\n",
        "    learning_rate=3e-4,\n",
        "    total_steps=total_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "0DxBfEE_c23p",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:03.027423Z",
          "iopub.execute_input": "2025-03-19T14:19:03.027639Z",
          "iopub.status.idle": "2025-03-19T14:19:07.576740Z",
          "shell.execute_reply.started": "2025-03-19T14:19:03.027617Z",
          "shell.execute_reply": "2025-03-19T14:19:07.575261Z"
        }
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "id": "taBtpmR_c6hP",
      "cell_type": "code",
      "source": [
        "rng_key = jax.random.PRNGKey(42)\n",
        "rng_key, init_key = jax.random.split(rng_key)\n",
        "train_state = create_train_state(init_key, config)\n",
        "\n",
        "# For TPU, replicate the state across devices\n",
        "if use_tpu:\n",
        "    train_state = replicate_state_on_devices(train_state)\n",
        "    # Create separate rng keys for each device\n",
        "    device_rng_keys = create_device_rng_keys(rng_key, num_devices)"
      ],
      "metadata": {
        "id": "taBtpmR_c6hP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:07.577642Z",
          "iopub.execute_input": "2025-03-19T14:19:07.577906Z",
          "iopub.status.idle": "2025-03-19T14:19:15.519529Z",
          "shell.execute_reply.started": "2025-03-19T14:19:07.577882Z",
          "shell.execute_reply": "2025-03-19T14:19:15.518251Z"
        }
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "id": "2yMh_joHdA0n",
      "cell_type": "code",
      "source": [
        "print(f\"Training for {total_steps} steps with batch size {batch_size}\")\n",
        "steps_to_run = min(5000, total_steps)  # Cap at 5000 steps for example\n",
        "\n",
        "for step in range(steps_to_run):\n",
        "    try:\n",
        "        # Get batch\n",
        "        if use_tpu:\n",
        "            # For TPU, create a batch for each device\n",
        "            x, y, rng_key = get_batch(rng_key, train_data, batch_size, config.block_size)\n",
        "            x, y = prepare_batch_for_devices((x, y), num_devices)\n",
        "\n",
        "            # Update device RNG keys correctly\n",
        "            device_rng_keys = step_device_rng_keys(device_rng_keys)\n",
        "\n",
        "            # Train step with parallel map\n",
        "            train_state, metrics, _ = train_step_pmap(train_state, (x, y), device_rng_keys, True)\n",
        "\n",
        "            # Extract metrics from first device (all are same due to pmean)\n",
        "            metrics = {k: v[0] for k, v in metrics.items()}\n",
        "        else:\n",
        "            # Standard single-device training\n",
        "            x, y, rng_key = get_batch(rng_key, train_data, batch_size, config.block_size)\n",
        "            train_state, metrics, _ = train_step(train_state, (x, y), rng_key)\n",
        "\n",
        "        # Print metrics occasionally\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}: loss = {metrics['loss']:.4f}, perplexity = {metrics['perplexity']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training step {step}: {e}\")\n",
        "        break"
      ],
      "metadata": {
        "id": "2yMh_joHdA0n",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:15.520309Z",
          "iopub.execute_input": "2025-03-19T14:19:15.520561Z",
          "iopub.status.idle": "2025-03-19T14:26:57.299546Z",
          "shell.execute_reply.started": "2025-03-19T14:19:15.520536Z",
          "shell.execute_reply": "2025-03-19T14:26:57.298503Z"
        },
        "outputId": "b1706ef4-33ea-45a8-ef2a-bb7a925f398b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 17645 steps with batch size 512\n",
            "Step 0: loss = 4.2805, perplexity = 72.2778\n",
            "Step 100: loss = 3.0267, perplexity = 20.6285\n",
            "Step 200: loss = 2.5807, perplexity = 13.2067\n",
            "Step 300: loss = 2.4765, perplexity = 11.9000\n",
            "Step 400: loss = 2.4099, perplexity = 11.1325\n",
            "Step 500: loss = 2.3279, perplexity = 10.2564\n",
            "Step 600: loss = 2.2291, perplexity = 9.2919\n",
            "Step 700: loss = 2.0846, perplexity = 8.0417\n",
            "Step 800: loss = 1.9728, perplexity = 7.1904\n",
            "Step 900: loss = 1.8547, perplexity = 6.3896\n",
            "Step 1000: loss = 1.7469, perplexity = 5.7367\n",
            "Step 1100: loss = 1.6489, perplexity = 5.2012\n",
            "Step 1200: loss = 1.5850, perplexity = 4.8792\n",
            "Step 1300: loss = 1.4981, perplexity = 4.4731\n",
            "Step 1400: loss = 1.4408, perplexity = 4.2242\n",
            "Step 1500: loss = 1.3872, perplexity = 4.0036\n",
            "Step 1600: loss = 1.3546, perplexity = 3.8753\n",
            "Step 1700: loss = 1.3225, perplexity = 3.7529\n",
            "Step 1800: loss = 1.2775, perplexity = 3.5875\n",
            "Step 1900: loss = 1.2395, perplexity = 3.4538\n",
            "Step 2000: loss = 1.2147, perplexity = 3.3692\n",
            "Step 2100: loss = 1.1836, perplexity = 3.2660\n",
            "Step 2200: loss = 1.1389, perplexity = 3.1232\n",
            "Step 2300: loss = 1.1171, perplexity = 3.0560\n",
            "Step 2400: loss = 1.0811, perplexity = 2.9480\n",
            "Step 2500: loss = 1.0579, perplexity = 2.8804\n",
            "Step 2600: loss = 1.0205, perplexity = 2.7747\n",
            "Step 2700: loss = 0.9929, perplexity = 2.6989\n",
            "Step 2800: loss = 0.9564, perplexity = 2.6022\n",
            "Step 2900: loss = 0.9390, perplexity = 2.5575\n",
            "Step 3000: loss = 0.9077, perplexity = 2.4787\n",
            "Step 3100: loss = 0.8702, perplexity = 2.3874\n",
            "Step 3200: loss = 0.8340, perplexity = 2.3024\n",
            "Step 3300: loss = 0.8171, perplexity = 2.2640\n",
            "Step 3400: loss = 0.7860, perplexity = 2.1946\n",
            "Step 3500: loss = 0.7534, perplexity = 2.1243\n",
            "Step 3600: loss = 0.7198, perplexity = 2.0540\n",
            "Step 3700: loss = 0.6951, perplexity = 2.0040\n",
            "Step 3800: loss = 0.6835, perplexity = 1.9809\n",
            "Step 3900: loss = 0.6512, perplexity = 1.9178\n",
            "Step 4000: loss = 0.6304, perplexity = 1.8783\n",
            "Step 4100: loss = 0.6064, perplexity = 1.8337\n",
            "Step 4200: loss = 0.5833, perplexity = 1.7919\n",
            "Step 4300: loss = 0.5723, perplexity = 1.7723\n",
            "Step 4400: loss = 0.5551, perplexity = 1.7421\n",
            "Step 4500: loss = 0.5308, perplexity = 1.7003\n",
            "Step 4600: loss = 0.5105, perplexity = 1.6662\n",
            "Step 4700: loss = 0.5080, perplexity = 1.6620\n",
            "Step 4800: loss = 0.4817, perplexity = 1.6189\n",
            "Step 4900: loss = 0.4746, perplexity = 1.6073\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "id": "7clu5fY4-6et",
      "cell_type": "markdown",
      "source": [
        "## Text Generation\n",
        "After training, we can generate text using the trained model. The generation process uses top-k sampling to produce coherent text.\n",
        "\n"
      ],
      "metadata": {
        "id": "7clu5fY4-6et"
      }
    },
    {
      "id": "f9KzvJ3adCpG",
      "cell_type": "code",
      "source": [
        "try:\n",
        "    prompt = \"ROMEO:\"\n",
        "    print(f\"Generating text from prompt: '{prompt}'\")\n",
        "    prompt_idx = jnp.array([encode(prompt)])\n",
        "\n",
        "    # If using TPU, get params from first device\n",
        "    if use_tpu:\n",
        "        params = jax.tree_util.tree_map(lambda x: x[0], train_state.params)\n",
        "        apply_fn = train_state.apply_fn\n",
        "    else:\n",
        "        params = train_state.params\n",
        "        apply_fn = train_state.apply_fn\n",
        "\n",
        "    rng_key, gen_key = jax.random.split(rng_key)\n",
        "    generated_idx = generate(\n",
        "        params,\n",
        "        apply_fn,\n",
        "        prompt_idx,\n",
        "        gen_key,\n",
        "        max_new_tokens=500,\n",
        "        temperature=0.8,\n",
        "        top_k=40,\n",
        "        block_size=config.block_size\n",
        "    )\n",
        "\n",
        "    generated_text = decode(generated_idx[0].tolist())\n",
        "    print(generated_text)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during text generation: {e}\")\n"
      ],
      "metadata": {
        "id": "f9KzvJ3adCpG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:26:57.301066Z",
          "iopub.execute_input": "2025-03-19T14:26:57.301435Z",
          "iopub.status.idle": "2025-03-19T14:39:18.888366Z",
          "shell.execute_reply.started": "2025-03-19T14:26:57.301384Z",
          "shell.execute_reply": "2025-03-19T14:39:18.887372Z"
        },
        "outputId": "e7b6aee2-9fad-4a71-9bc9-7c11fd7d3f99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text from prompt: 'ROMEO:'\n",
            "ROMEO:\n",
            "By that's a traitor to thee.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Not in a grave,\n",
            "To leave me speak and to loving love,\n",
            "Think me and spring, ungracious brows,\n",
            "And take not this certain despite of all\n",
            "The people of this common monument.\n",
            "To Bolingbroke, fellow-morrow,\n",
            "That it shall rest out with far off heaven and kind\n",
            "The rebels and smother's angel things present,\n",
            "The which horse that did got the poison slaughter\n",
            "With that thus have true.\n",
            "\n",
            "CORIOLANUS:\n",
            "Come, ladies,\n",
            "To mock up some place, chastise the noble duke.\n",
            "\n",
            "A\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Model\n",
        "We can export the model out to save the parameters to save time in the future\n"
      ],
      "metadata": {
        "id": "YwP6uvfliEfD"
      },
      "id": "YwP6uvfliEfD"
    },
    {
      "cell_type": "code",
      "source": [
        "def export_model(state, config, charset, checkpoint_dir=\"model_export\"):\n",
        "    \"\"\"\n",
        "    Export the model parameters, configuration, and character set for later use.\n",
        "\n",
        "    Args:\n",
        "        state: The training state containing model parameters\n",
        "        config: The TransformerConfig instance used for the model\n",
        "        charset: Dictionary mapping between characters and indices or the raw character list\n",
        "        checkpoint_dir: Directory to save the exported model\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pickle\n",
        "    import json\n",
        "\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Extract parameters from state (handling both TPU and CPU/GPU cases)\n",
        "    if isinstance(state.params, list):\n",
        "        # For TPU: extract params from first device\n",
        "        params = jax.tree_util.tree_map(lambda x: x[0], state.params)\n",
        "    else:\n",
        "        params = state.params\n",
        "\n",
        "    # Save model parameters\n",
        "    with open(os.path.join(checkpoint_dir, \"model_params.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(params, f)\n",
        "\n",
        "    # Save config as JSON\n",
        "    config_dict = config.__dict__\n",
        "    with open(os.path.join(checkpoint_dir, \"config.json\"), \"w\") as f:\n",
        "        json.dump(config_dict, f, indent=2)\n",
        "\n",
        "    # Save character set (handling different formats)\n",
        "    if isinstance(charset, dict):\n",
        "        # If charset is already a dict mapping chars to indices\n",
        "        char_mapping = charset\n",
        "    elif isinstance(charset, list):\n",
        "        # If charset is a list of characters\n",
        "        char_mapping = {ch: i for i, ch in enumerate(charset)}\n",
        "    else:\n",
        "        raise ValueError(\"charset must be either a dict mapping or a list of characters\")\n",
        "\n",
        "    with open(os.path.join(checkpoint_dir, \"charset.json\"), \"w\") as f:\n",
        "        json.dump(char_mapping, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Model successfully exported to {checkpoint_dir}\")\n",
        "    return checkpoint_dir\n",
        "\n",
        "def load_model(checkpoint_dir=\"model_export\"):\n",
        "    \"\"\"\n",
        "    Load a saved model for inference.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_dir: Directory containing the exported model\n",
        "\n",
        "    Returns:\n",
        "        params: The model parameters\n",
        "        config: Reconstructed TransformerConfig\n",
        "        encode_fn: Function to encode text to token indices\n",
        "        decode_fn: Function to decode token indices to text\n",
        "        model: Instantiated GPT model\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import pickle\n",
        "    import json\n",
        "    import jax\n",
        "\n",
        "    # Load model parameters\n",
        "    with open(os.path.join(checkpoint_dir, \"model_params.pkl\"), \"rb\") as f:\n",
        "        params = pickle.load(f)\n",
        "\n",
        "    # Load config\n",
        "    with open(os.path.join(checkpoint_dir, \"config.json\"), \"r\") as f:\n",
        "        config_dict = json.load(f)\n",
        "\n",
        "    # Reconstruct TransformerConfig\n",
        "    config = TransformerConfig(**config_dict)\n",
        "\n",
        "    # Load character set\n",
        "    with open(os.path.join(checkpoint_dir, \"charset.json\"), \"r\") as f:\n",
        "        stoi = json.load(f)\n",
        "\n",
        "    # Create inverse mapping (index to character)\n",
        "    itos = {int(i): ch for ch, i in stoi.items()}\n",
        "\n",
        "    # Create encode/decode functions\n",
        "    encode_fn = lambda s: [stoi[c] for c in s if c in stoi]\n",
        "    decode_fn = lambda l: ''.join([itos[i] for i in l if i in itos])\n",
        "\n",
        "    # Create model instance\n",
        "    model = GPT(config)\n",
        "\n",
        "    print(f\"Model successfully loaded from {checkpoint_dir}\")\n",
        "    return params, config, encode_fn, decode_fn, model\n",
        "\n",
        "def generate_text(params, model, prompt, encode_fn, decode_fn, config,\n",
        "                  max_new_tokens=100, temperature=0.8, top_k=40):\n",
        "    \"\"\"\n",
        "    Generate text using a loaded model.\n",
        "\n",
        "    Args:\n",
        "        params: Model parameters\n",
        "        model: GPT model instance\n",
        "        prompt: Text prompt to continue from\n",
        "        encode_fn: Function to encode text to token indices\n",
        "        decode_fn: Function to decode token indices to text\n",
        "        config: TransformerConfig for the model\n",
        "        max_new_tokens: Maximum number of tokens to generate\n",
        "        temperature: Temperature for sampling (higher = more random)\n",
        "        top_k: Number of top tokens to consider for sampling\n",
        "\n",
        "    Returns:\n",
        "        generated_text: The complete generated text including the prompt\n",
        "    \"\"\"\n",
        "    import jax\n",
        "    import jax.numpy as jnp\n",
        "\n",
        "    # Encode the prompt\n",
        "    prompt_idx = jnp.array([encode_fn(prompt)])\n",
        "\n",
        "    # Create RNG key\n",
        "    rng_key = jax.random.PRNGKey(42)\n",
        "\n",
        "    # Generate text\n",
        "    generated_idx = generate(\n",
        "        params,\n",
        "        model.apply,\n",
        "        prompt_idx,\n",
        "        rng_key,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        block_size=config.block_size\n",
        "    )\n",
        "\n",
        "    # Decode to text\n",
        "    generated_text = decode_fn(generated_idx[0].tolist())\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "As8DVUG_h4N6"
      },
      "id": "As8DVUG_h4N6",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_model(train_state, config, chars, checkpoint_dir=\"shakespeare_model\")\n",
        "\n",
        "# Later, for inference:\n",
        "params, config, encode_fn, decode_fn, model = load_model(checkpoint_dir=\"shakespeare_model\")"
      ],
      "metadata": {
        "id": "wwjGDDZYiKD_",
        "outputId": "afd4db17-ea44-4e4b-df4c-43ebe5c9684f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wwjGDDZYiKD_",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model successfully exported to shakespeare_model\n",
            "Model successfully loaded from shakespeare_model\n"
          ]
        }
      ]
    }
  ]
}