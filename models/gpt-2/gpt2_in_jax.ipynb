{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8b39517e",
      "metadata": {
        "id": "8b39517e"
      },
      "source": [
        "\n",
        "# GPT-2 JAX Implementation\n",
        "\n",
        "This notebook demonstrates how to use GPT-2 with JAX for efficient text generation.\n",
        "It includes model loading, inference, and text generation using JAX-based tensor computations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80b92b23",
      "metadata": {
        "id": "80b92b23"
      },
      "source": [
        "## Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "631acac9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "631acac9",
        "outputId": "b8874493-eb59-4dce-d2df-02b4b053926b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"karpathy/tiny_shakespeare\"\n",
        "\"\"\"\n",
        "Other examples may include :\n",
        "[Note that these may take longer]\n",
        "\n",
        "dataset_name = \"mindchain/wikitext2\"  # WikiText dataset\n",
        "dataset_name = \"SamuelYang/bookcorpus\"  # BookCorpus dataset\n",
        "dataset_name = \"oscar-corpus/oscar\"  # OSCAR dataset\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tdWXHg02ozPk",
        "outputId": "b0dfad16-f9c5-4640-8bbe-6688123c5997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "id": "tdWXHg02ozPk",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOther examples may include : \\n[Note that these may take longer]\\n\\ndataset_name = \"mindchain/wikitext2\"  # WikiText dataset\\ndataset_name = \"SamuelYang/bookcorpus\"  # BookCorpus dataset\\ndataset_name = \"oscar-corpus/oscar\"  # OSCAR dataset\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "from typing import Any, Callable, Dict, Optional, Tuple\n",
        "from flax.training import train_state\n",
        "from functools import partial\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8qohV72gSRbO"
      },
      "id": "8qohV72gSRbO",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3e1188",
      "metadata": {
        "id": "aa3e1188"
      },
      "outputs": [],
      "source": [
        "class TransformerConfig:\n",
        "    \"\"\"Configuration for the transformer model.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        block_size: int = 256,\n",
        "        n_embed: int = 384,\n",
        "        n_head: int = 6,\n",
        "        n_layer: int = 6,\n",
        "        dropout: float = 0.2,\n",
        "        learning_rate: float = 3e-4,\n",
        "        weight_decay: float = 0.1,\n",
        "        beta1: float = 0.9,\n",
        "        beta2: float = 0.95,\n",
        "        grad_clip: float = 1.0,\n",
        "        warmup_steps: int = 2000,\n",
        "        total_steps: int = 250000,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_embed = n_embed\n",
        "        self.n_head = n_head\n",
        "        self.n_layer = n_layer\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.grad_clip = grad_clip\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = total_steps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainState(train_state.TrainState):\n",
        "    \"\"\"Custom train state with additional fields if needed.\"\"\"\n",
        "    # Add any additional fields here if necessary\n",
        "    pass\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head causal self-attention with optimized implementation.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        config = self.config\n",
        "        assert config.n_embed % config.n_head == 0\n",
        "\n",
        "        # Key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Dense(\n",
        "            3 * config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Dense(\n",
        "            config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "\n",
        "        # Causal mask to ensure attention only to previous tokens\n",
        "        # Instead of register_buffer, we'll make it a class variable\n",
        "        self.bias = jnp.tril(jnp.ones((config.block_size, config.block_size)))\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed = config.n_embed\n",
        "        self.head_dim = config.n_embed // config.n_head\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        B, T, C = x.shape  # batch size, sequence length, embedding dim\n",
        "\n",
        "        # Calculate query, key, values for all heads in batch\n",
        "        q, k, v = jnp.split(self.c_attn(x), 3, axis=-1)\n",
        "\n",
        "        # Reshape to (B, T, nh, hs)\n",
        "        k = k.reshape(B, T, self.n_head, self.head_dim)\n",
        "        q = q.reshape(B, T, self.n_head, self.head_dim)\n",
        "        v = v.reshape(B, T, self.n_head, self.head_dim)\n",
        "\n",
        "        # Transpose to (B, nh, T, hs)\n",
        "        k = jnp.transpose(k, (0, 2, 1, 3))\n",
        "        q = jnp.transpose(q, (0, 2, 1, 3))\n",
        "        v = jnp.transpose(v, (0, 2, 1, 3))\n",
        "\n",
        "        # Efficient scaled dot-product attention\n",
        "        scale = jnp.sqrt(self.head_dim)\n",
        "        att = (q @ jnp.transpose(k, (0, 1, 3, 2))) / scale  # (B, nh, T, T)\n",
        "\n",
        "        # Causal mask to ensure attention only to past tokens\n",
        "        mask = self.bias[:T, :T]\n",
        "        # Use jnp.where with a mask for better TPU compatibility\n",
        "        att = jnp.where(mask == 0, jnp.full_like(att, -1e10), att)  # (B, nh, T, T)\n",
        "\n",
        "        # Softmax attention\n",
        "        att = jax.nn.softmax(att, axis=-1)\n",
        "        att = self.attn_dropout(att, deterministic=not training)\n",
        "\n",
        "        # Combine heads\n",
        "        y = att @ v  # (B, nh, T, hs)\n",
        "        y = jnp.transpose(y, (0, 2, 1, 3))  # (B, T, nh, hs)\n",
        "        y = y.reshape(B, T, C)  # (B, T, C)\n",
        "\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y), deterministic=not training)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"MLP with better initialization and gelu activation.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        config = self.config\n",
        "        self.c_fc = nn.Dense(\n",
        "            4 * config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "        # Use approximate=True for faster GELU on TPUs\n",
        "        self.gelu = lambda x: jax.nn.gelu(x, approximate=True)\n",
        "        self.c_proj = nn.Dense(\n",
        "            config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block with pre-layer normalization.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        self.ln_1 = nn.LayerNorm(epsilon=1e-5)\n",
        "        self.attn = CausalSelfAttention(self.config)\n",
        "        self.ln_2 = nn.LayerNorm(epsilon=1e-5)\n",
        "        self.mlp = MLP(self.config)\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        # Pre-layer normalization design\n",
        "        x = x + self.attn(self.ln_1(x), training=training)\n",
        "        x = x + self.mlp(self.ln_2(x), training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"GPT Language Model with improved implementation.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        config = self.config\n",
        "\n",
        "        # Token and position embeddings\n",
        "        self.wte = nn.Embed(\n",
        "            config.vocab_size,\n",
        "            config.n_embed,\n",
        "            embedding_init=nn.initializers.normal(stddev=0.02)\n",
        "        )\n",
        "        self.wpe = nn.Embed(\n",
        "            config.block_size,\n",
        "            config.n_embed,\n",
        "            embedding_init=nn.initializers.normal(stddev=0.02)\n",
        "        )\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = [Block(config) for _ in range(config.n_layer)]\n",
        "\n",
        "        # Final layer norm and head\n",
        "        self.ln_f = nn.LayerNorm(epsilon=1e-5)\n",
        "        self.lm_head = nn.Dense(\n",
        "            config.vocab_size,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros,\n",
        "            use_bias=False\n",
        "        )\n",
        "\n",
        "        # For weight tying\n",
        "        self.apply_weight_tying = True\n",
        "\n",
        "    def _tie_weights(self, params):\n",
        "        \"\"\"Tie embedding weights with output layer if enabled.\"\"\"\n",
        "        if not self.apply_weight_tying:\n",
        "            return params\n",
        "\n",
        "        # Clone the parameters and update lm_head kernel with embedding weights\n",
        "        new_params = params.copy()\n",
        "        new_params['lm_head']['kernel'] = new_params['wte']['embedding']\n",
        "        return new_params\n",
        "\n",
        "    def __call__(self, idx, targets=None, training=False, params=None):\n",
        "        config = self.config\n",
        "        b, t = idx.shape\n",
        "\n",
        "        # Apply weight tying if enabled (only during inference)\n",
        "        if params is not None and self.apply_weight_tying and not training:\n",
        "            params = self._tie_weights(params)\n",
        "\n",
        "        # Get token and position embeddings\n",
        "        token_emb = self.wte(idx)  # (b, t, n_embed)\n",
        "        pos = jnp.arange(0, t, dtype=jnp.int32)\n",
        "        pos_emb = self.wpe(pos)  # (t, n_embed)\n",
        "\n",
        "        # Sum embeddings and apply dropout\n",
        "        x = token_emb + pos_emb\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        # Apply final layer norm\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        logits = self.lm_head(x)  # (b, t, vocab_size)\n",
        "\n",
        "        # If targets are provided, compute loss\n",
        "        if targets is not None:\n",
        "            # Cross-entropy loss for next token prediction\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "                logits.reshape(-1, config.vocab_size),\n",
        "                targets.reshape(-1)\n",
        "            ).mean()\n",
        "            return logits, loss\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ggUDGuSqR7FJ"
      },
      "id": "ggUDGuSqR7FJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Data Loading ----------\n",
        "def get_batch(rng_key, data, batch_size, block_size):\n",
        "    \"\"\"Get a random batch of data with safety checks.\"\"\"\n",
        "    data_size = data.shape[0]\n",
        "\n",
        "    # Safety check: ensure data is large enough\n",
        "    if data_size <= block_size:\n",
        "        raise ValueError(f\"Data size ({data_size}) must be larger than block size ({block_size})\")\n",
        "\n",
        "    rng_key, split_key = jax.random.split(rng_key)\n",
        "\n",
        "    # Use jax.random.randint for generating random indices\n",
        "    max_start_idx = data_size - block_size - 1\n",
        "    indices = jax.random.randint(\n",
        "        split_key,\n",
        "        shape=(batch_size,),\n",
        "        minval=0,\n",
        "        maxval=max_start_idx\n",
        "    )\n",
        "\n",
        "    # TPU-optimized data loading using vectorized operations\n",
        "    idx = jnp.arange(block_size)\n",
        "    offsets = indices[:, None]\n",
        "    x_indices = offsets + idx\n",
        "    y_indices = offsets + idx + 1\n",
        "\n",
        "    x = jnp.take(data, x_indices, axis=0)\n",
        "    y = jnp.take(data, y_indices, axis=0)\n",
        "\n",
        "    return x, y, rng_key\n"
      ],
      "metadata": {
        "id": "onvhoeOfSAjX"
      },
      "id": "onvhoeOfSAjX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_state(rng_key, config):\n",
        "    \"\"\"Create initial training state.\"\"\"\n",
        "    model = GPT(config)\n",
        "\n",
        "    # Initialize model parameters with a properly shaped input\n",
        "    dummy_input = jnp.ones((8, 64), dtype=jnp.int32)  # Use batch size divisible by 8 for TPU\n",
        "    params = model.init(rng_key, dummy_input, training=False)[\"params\"]\n",
        "\n",
        "    # Create optimizer with learning rate schedule\n",
        "    lr_schedule = optax.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=config.learning_rate,\n",
        "        warmup_steps=config.warmup_steps,\n",
        "        decay_steps=config.total_steps,\n",
        "        end_value=config.learning_rate * 0.1,\n",
        "    )\n",
        "\n",
        "    # AdamW optimizer with weight decay and gradient clipping\n",
        "    optimizer = optax.chain(\n",
        "        optax.clip_by_global_norm(config.grad_clip),\n",
        "        optax.adamw(\n",
        "            learning_rate=lr_schedule,\n",
        "            b1=config.beta1,\n",
        "            b2=config.beta2,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return CustomTrainState.create(\n",
        "        apply_fn=model.apply,\n",
        "        params=params,\n",
        "        tx=optimizer\n",
        "    )\n",
        "\n",
        "# TPU-optimized training step with pmap support\n",
        "@partial(jax.pmap, axis_name='batch', static_broadcasted_argnums=(3,))\n",
        "def train_step_pmap(state, batch, rng_keys, training=True):\n",
        "    \"\"\"Single training step with parallel processing support for TPUs.\"\"\"\n",
        "    inputs, targets = batch\n",
        "\n",
        "    # Use different dropout key for each step and device\n",
        "    dropout_rng = jax.random.fold_in(\n",
        "        rng_keys,\n",
        "        state.step\n",
        "    )\n",
        "\n",
        "    def loss_fn(params):\n",
        "        logits, loss = state.apply_fn(\n",
        "            {\"params\": params},\n",
        "            inputs,\n",
        "            targets=targets,\n",
        "            training=training,\n",
        "            rngs={\"dropout\": dropout_rng}\n",
        "        )\n",
        "        return loss, logits\n",
        "\n",
        "    # Compute loss and gradients\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(state.params)\n",
        "\n",
        "    # Average gradients across replicas\n",
        "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
        "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
        "\n",
        "    # Update model parameters\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "\n",
        "    return new_state, metrics, logits\n",
        "\n",
        "\n",
        "# Standard training step for single device\n",
        "@partial(jax.jit, static_argnums=(3,))\n",
        "def train_step(state, batch, rng_key, training=True):\n",
        "    \"\"\"Single training step for single device.\"\"\"\n",
        "    inputs, targets = batch\n",
        "\n",
        "    # Use different dropout key for each step\n",
        "    dropout_rng = jax.random.fold_in(rng_key, state.step)\n",
        "\n",
        "    def loss_fn(params):\n",
        "        logits, loss = state.apply_fn(\n",
        "            {\"params\": params},\n",
        "            inputs,\n",
        "            targets=targets,\n",
        "            training=training,\n",
        "            rngs={\"dropout\": dropout_rng}\n",
        "        )\n",
        "        return loss, logits\n",
        "\n",
        "    # Compute loss and gradients\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(state.params)\n",
        "\n",
        "    # Update model parameters\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "\n",
        "    return new_state, metrics, logits\n",
        "\n",
        "\n",
        "# TPU-optimized evaluation step with pmap support\n",
        "@partial(jax.pmap, axis_name='batch', static_broadcasted_argnums=(2,))\n",
        "def eval_step_pmap(state, batch, training=False):\n",
        "    \"\"\"Evaluation step with parallel processing support for TPUs.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    logits, loss = state.apply_fn(\n",
        "        {\"params\": state.params},\n",
        "        inputs,\n",
        "        targets=targets,\n",
        "        training=training,\n",
        "    )\n",
        "\n",
        "    # Average loss across replicas\n",
        "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Standard evaluation step for single device\n",
        "@partial(jax.jit, static_argnums=(2,))\n",
        "def eval_step(state, batch, training=False):\n",
        "    \"\"\"Evaluation step for single device.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    logits, loss = state.apply_fn(\n",
        "        {\"params\": state.params},\n",
        "        inputs,\n",
        "        targets=targets,\n",
        "        training=training,\n",
        "    )\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "3uCdC307SGXG"
      },
      "id": "3uCdC307SGXG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(jax.jit, static_argnums=(2, 3, 4, 5))\n",
        "def generate_step(params, idx, block_size, temperature=1.0, top_k=40, apply_fn=None, rng_key=None):\n",
        "    \"\"\"Single generation step using top-k sampling.\"\"\"\n",
        "    # Take the last block_size tokens as context (or fewer if not enough)\n",
        "    context_size = min(idx.shape[1], block_size)\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "\n",
        "    # Get logits from the model\n",
        "    logits = apply_fn({\"params\": params}, idx_cond, training=False)\n",
        "\n",
        "    # Focus only on the last time step\n",
        "    logits = logits[:, -1, :] / temperature\n",
        "\n",
        "    # Optional top-k sampling using JAX's efficient operators\n",
        "    if top_k > 0:\n",
        "        top_k = min(top_k, logits.shape[-1])\n",
        "        topk_values, _ = jax.lax.top_k(logits, top_k)\n",
        "        threshold = topk_values[:, -1]\n",
        "        logits = jnp.where(logits < threshold[:, None], jnp.full_like(logits, -1e10), logits)\n",
        "\n",
        "    # Sample from the distribution\n",
        "    sample = jax.random.categorical(rng_key, logits, axis=-1)\n",
        "\n",
        "    # Append to the sequence\n",
        "    return jnp.concatenate([idx, sample[:, None]], axis=1)\n",
        "\n",
        "\n",
        "def generate(\n",
        "    params,\n",
        "    apply_fn,\n",
        "    prompt_idx,\n",
        "    rng_key,\n",
        "    max_new_tokens=100,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    block_size=256\n",
        "):\n",
        "    \"\"\"Generate text using the model with proper handling of long prompts.\"\"\"\n",
        "    # Handle prompt that may be longer than block_size\n",
        "    if prompt_idx.shape[1] > block_size:\n",
        "        # Only keep the last block_size tokens of the prompt\n",
        "        idx = prompt_idx[:, -block_size:]\n",
        "        print(f\"Warning: Prompt was truncated to the last {block_size} tokens due to context length limit.\")\n",
        "    else:\n",
        "        idx = prompt_idx\n",
        "\n",
        "    # Generate tokens one by one\n",
        "    for _ in range(max_new_tokens):\n",
        "        rng_key, next_key = jax.random.split(rng_key)\n",
        "        idx = generate_step(\n",
        "            params,\n",
        "            idx,\n",
        "            block_size,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            apply_fn=apply_fn,\n",
        "            rng_key=next_key\n",
        "        )\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "# ---------- TPU Initialization and Multi-device Training ----------\n",
        "def initialize_tpu():\n",
        "    \"\"\"Initialize TPU system.\"\"\"\n",
        "    # Check if running on TPU\n",
        "    if 'tpu' in jax.devices()[0].platform:\n",
        "        print(f\"Running on {jax.device_count()} TPU devices\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Not running on TPU\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def replicate_state_on_devices(state):\n",
        "    \"\"\"Replicate state across all TPU devices.\"\"\"\n",
        "    # Broadcast the state to all devices\n",
        "    state = jax.device_put_replicated(state, jax.local_devices())\n",
        "    return state\n",
        "\n",
        "\n",
        "def prepare_batch_for_devices(batch, num_devices):\n",
        "    \"\"\"Prepare batch for multiple devices by reshaping.\"\"\"\n",
        "    x, y = batch\n",
        "\n",
        "    # Get total batch size and calculate per-device batch size\n",
        "    total_batch_size = x.shape[0]\n",
        "    per_device_batch_size = total_batch_size // num_devices\n",
        "\n",
        "    # Reshape to (num_devices, per_device_batch_size, ...)\n",
        "    x = x.reshape((num_devices, per_device_batch_size) + x.shape[1:])\n",
        "    y = y.reshape((num_devices, per_device_batch_size) + y.shape[1:])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def create_device_rng_keys(rng_key, num_devices):\n",
        "    \"\"\"Create separate RNG keys for each device.\"\"\"\n",
        "    # Split the main key into num_devices keys\n",
        "    return jax.random.split(rng_key, num_devices)\n",
        "\n",
        "\n",
        "def step_device_rng_keys(rng_keys):\n",
        "    \"\"\"Update RNG keys for each device independently.\"\"\"\n",
        "    # Split each device's key to get a new key for each device\n",
        "    new_keys = jax.vmap(jax.random.split)(rng_keys)\n",
        "    # Each split returns 2 keys, keep the first one for each device\n",
        "    return new_keys[:, 0]"
      ],
      "metadata": {
        "id": "up4L7GuoSeXs"
      },
      "id": "up4L7GuoSeXs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset_name, split=\"train\"):\n",
        "    \"\"\"\n",
        "    Prepare a Hugging Face dataset for training.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: str, name of the dataset on Hugging Face\n",
        "        split: str, which split to use (default: \"train\")\n",
        "\n",
        "    Returns:\n",
        "        train_data: encoded training data\n",
        "        val_data: encoded validation data\n",
        "        encode_fn: function to encode text\n",
        "        decode_fn: function to decode indices\n",
        "        vocab_size: size of vocabulary\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(dataset_name)\n",
        "\n",
        "    # Get text from dataset\n",
        "    if \"text\" in dataset[split].features:\n",
        "        text_key = \"text\"\n",
        "    else:\n",
        "        # Try to find a text field\n",
        "        text_fields = [k for k, v in dataset[split].features.items()\n",
        "                      if v.dtype == 'string']\n",
        "        if text_fields:\n",
        "            text_key = text_fields[0]\n",
        "        else:\n",
        "            raise ValueError(\"Could not find text field in dataset\")\n",
        "\n",
        "    # Combine all texts\n",
        "    text = \"\\n\".join(dataset[split][text_key])\n",
        "\n",
        "    # Create vocabulary\n",
        "    chars = sorted(list(set(text)))\n",
        "    vocab_size = len(chars)\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    # Create encode/decode functions\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "    # Encode full text\n",
        "    data = jnp.array(encode(text))\n",
        "\n",
        "    # Split into train/val\n",
        "    n = int(0.9 * len(data))\n",
        "    train_data = data[:n]\n",
        "    val_data = data[n:]\n",
        "\n",
        "    return train_data, val_data, encode, decode, vocab_size"
      ],
      "metadata": {
        "id": "mISHmd0eoap9"
      },
      "id": "mISHmd0eoap9",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_tpu = initialize_tpu()\n",
        "num_devices = jax.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXDYFJ93Shje",
        "outputId": "244e0f27-f687-4d3a-e6ea-62287d302117"
      },
      "id": "vXDYFJ93Shje",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on 8 TPU devices\n",
            "Training for 19606 steps with batch size 512\n",
            "Step 0: loss = 4.2834, perplexity = 72.4865\n",
            "Step 100: loss = 3.0169, perplexity = 20.4285\n",
            "Step 200: loss = 2.5909, perplexity = 13.3414\n",
            "Step 300: loss = 2.4781, perplexity = 11.9187\n",
            "Step 400: loss = 2.4098, perplexity = 11.1313\n",
            "Step 500: loss = 2.3361, perplexity = 10.3410\n",
            "Step 600: loss = 2.2234, perplexity = 9.2391\n",
            "Step 700: loss = 2.1081, perplexity = 8.2323\n",
            "Step 800: loss = 1.9790, perplexity = 7.2356\n",
            "Step 900: loss = 1.8747, perplexity = 6.5191\n",
            "Step 1000: loss = 1.7532, perplexity = 5.7732\n",
            "Step 1100: loss = 1.6494, perplexity = 5.2040\n",
            "Step 1200: loss = 1.5778, perplexity = 4.8441\n",
            "Step 1300: loss = 1.5055, perplexity = 4.5062\n",
            "Step 1400: loss = 1.4530, perplexity = 4.2757\n",
            "Step 1500: loss = 1.4057, perplexity = 4.0783\n",
            "Step 1600: loss = 1.3573, perplexity = 3.8857\n",
            "Step 1700: loss = 1.3261, perplexity = 3.7663\n",
            "Step 1800: loss = 1.2923, perplexity = 3.6411\n",
            "Step 1900: loss = 1.2524, perplexity = 3.4989\n",
            "Step 2000: loss = 1.2180, perplexity = 3.3803\n",
            "Step 2100: loss = 1.2034, perplexity = 3.3313\n",
            "Step 2200: loss = 1.1707, perplexity = 3.2242\n",
            "Step 2300: loss = 1.1394, perplexity = 3.1248\n",
            "Step 2400: loss = 1.1017, perplexity = 3.0094\n",
            "Step 2500: loss = 1.0916, perplexity = 2.9790\n",
            "Step 2600: loss = 1.0628, perplexity = 2.8946\n",
            "Step 2700: loss = 1.0257, perplexity = 2.7891\n",
            "Step 2800: loss = 1.0057, perplexity = 2.7338\n",
            "Step 2900: loss = 0.9665, perplexity = 2.6286\n",
            "Step 3000: loss = 0.9542, perplexity = 2.5966\n",
            "Step 3100: loss = 0.9207, perplexity = 2.5111\n",
            "Step 3200: loss = 0.8921, perplexity = 2.4401\n",
            "Step 3300: loss = 0.8689, perplexity = 2.3843\n",
            "Step 3400: loss = 0.8430, perplexity = 2.3232\n",
            "Step 3500: loss = 0.8069, perplexity = 2.2410\n",
            "Step 3600: loss = 0.7818, perplexity = 2.1855\n",
            "Step 3700: loss = 0.7568, perplexity = 2.1315\n",
            "Step 3800: loss = 0.7438, perplexity = 2.1039\n",
            "Step 3900: loss = 0.7298, perplexity = 2.0746\n",
            "Step 4000: loss = 0.6994, perplexity = 2.0126\n",
            "Step 4100: loss = 0.6671, perplexity = 1.9487\n",
            "Step 4200: loss = 0.6710, perplexity = 1.9562\n",
            "Step 4300: loss = 0.6435, perplexity = 1.9031\n",
            "Step 4400: loss = 0.6223, perplexity = 1.8633\n",
            "Step 4500: loss = 0.6071, perplexity = 1.8351\n",
            "Step 4600: loss = 0.5919, perplexity = 1.8075\n",
            "Step 4700: loss = 0.5697, perplexity = 1.7677\n",
            "Step 4800: loss = 0.5603, perplexity = 1.7512\n",
            "Step 4900: loss = 0.5468, perplexity = 1.7277\n",
            "Generating text from prompt: 'ROMEO:'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-1d79001fb911>:112: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  params = jax.tree_map(lambda x: x[0], train_state.params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "By heaven, I'll steal answer.\n",
            "\n",
            "PETRUCHIO:\n",
            "I pray you, daughter, sit by my fault,\n",
            "I will have you utter for the maid:\n",
            "Therefore I shall never remember it.\n",
            "\n",
            "GRUMIO:\n",
            "And if you live, pray you, let me have you do\n",
            "And I'll do it at your heart.\n",
            "\n",
            "PERDITA:\n",
            "I shall believe this grace in mine opinion,\n",
            "And then dreams you have done.\n",
            "\n",
            "CORIOLANUS:\n",
            "I dare be sworn you,\n",
            "And speak but the socrate of us, when you partly\n",
            "Should know I am you subject.\n",
            "\n",
            "SICINIUS:\n",
            "I am about,\n",
            "As you have said trifles.\n",
            "\n",
            "MENENIUS:\n",
            "Co\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from Hugging Face\n",
        "train_data, val_data, encode, decode, vocab_size = prepare_dataset(dataset_name)\n",
        "# Calculate reasonable total_steps based on dataset size\n",
        "data_size = len(train_data)\n",
        "batch_size = min(64, data_size // 10)  # Cap batch size at 64\n",
        "if use_tpu:\n",
        "    # Scale batch size responsibly\n",
        "    devices_to_use = min(num_devices, 8)  # Cap device usage if too many\n",
        "    batch_size = batch_size * devices_to_use\n",
        "\n",
        "# Estimate total steps based on dataset size and batch size\n",
        "# Aim for ~100 epochs for small datasets, fewer for larger ones\n",
        "epochs = max(10, min(100, 1000000 // data_size))\n",
        "total_steps = (data_size * epochs) // batch_size\n",
        "\n",
        "config = TransformerConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=min(256, data_size // 2),  # Ensure block_size isn't too large\n",
        "    n_embed=384,\n",
        "    n_head=6,\n",
        "    n_layer=6,\n",
        "    dropout=0.2,\n",
        "    learning_rate=3e-4,\n",
        "    total_steps=total_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "0DxBfEE_c23p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "165665eb-daff-42bf-aeac-4d3eb117e110"
      },
      "id": "0DxBfEE_c23p",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'use_tpu' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2c7555b0b862>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Cap batch size at 64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Scale batch size responsibly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdevices_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_devices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Cap device usage if too many\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'use_tpu' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rng_key = jax.random.PRNGKey(42)\n",
        "rng_key, init_key = jax.random.split(rng_key)\n",
        "train_state = create_train_state(init_key, config)\n",
        "\n",
        "# For TPU, replicate the state across devices\n",
        "if use_tpu:\n",
        "    train_state = replicate_state_on_devices(train_state)\n",
        "    # Create separate rng keys for each device\n",
        "    device_rng_keys = create_device_rng_keys(rng_key, num_devices)"
      ],
      "metadata": {
        "id": "taBtpmR_c6hP"
      },
      "id": "taBtpmR_c6hP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training for {total_steps} steps with batch size {batch_size}\")\n",
        "steps_to_run = min(5000, total_steps)  # Cap at 5000 steps for example\n",
        "\n",
        "for step in range(steps_to_run):\n",
        "    try:\n",
        "        # Get batch\n",
        "        if use_tpu:\n",
        "            # For TPU, create a batch for each device\n",
        "            x, y, rng_key = get_batch(rng_key, train_data, batch_size, config.block_size)\n",
        "            x, y = prepare_batch_for_devices((x, y), num_devices)\n",
        "\n",
        "            # Update device RNG keys correctly\n",
        "            device_rng_keys = step_device_rng_keys(device_rng_keys)\n",
        "\n",
        "            # Train step with parallel map\n",
        "            train_state, metrics, _ = train_step_pmap(train_state, (x, y), device_rng_keys, True)\n",
        "\n",
        "            # Extract metrics from first device (all are same due to pmean)\n",
        "            metrics = {k: v[0] for k, v in metrics.items()}\n",
        "        else:\n",
        "            # Standard single-device training\n",
        "            x, y, rng_key = get_batch(rng_key, train_data, batch_size, config.block_size)\n",
        "            train_state, metrics, _ = train_step(train_state, (x, y), rng_key)\n",
        "\n",
        "        # Print metrics occasionally\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}: loss = {metrics['loss']:.4f}, perplexity = {metrics['perplexity']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training step {step}: {e}\")\n",
        "        break"
      ],
      "metadata": {
        "id": "2yMh_joHdA0n"
      },
      "id": "2yMh_joHdA0n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    prompt = \"ROMEO:\"\n",
        "    print(f\"Generating text from prompt: '{prompt}'\")\n",
        "    prompt_idx = jnp.array([encode(prompt)])\n",
        "\n",
        "    # If using TPU, get params from first device\n",
        "    if use_tpu:\n",
        "        params = jax.tree_map(lambda x: x[0], train_state.params)\n",
        "        apply_fn = train_state.apply_fn\n",
        "    else:\n",
        "        params = train_state.params\n",
        "        apply_fn = train_state.apply_fn\n",
        "\n",
        "    rng_key, gen_key = jax.random.split(rng_key)\n",
        "    generated_idx = generate(\n",
        "        params,\n",
        "        apply_fn,\n",
        "        prompt_idx,\n",
        "        gen_key,\n",
        "        max_new_tokens=500,\n",
        "        temperature=0.8,\n",
        "        top_k=40,\n",
        "        block_size=config.block_size\n",
        "    )\n",
        "\n",
        "    generated_text = decode(generated_idx[0].tolist())\n",
        "    print(generated_text)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during text generation: {e}\")\n"
      ],
      "metadata": {
        "id": "f9KzvJ3adCpG"
      },
      "id": "f9KzvJ3adCpG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}