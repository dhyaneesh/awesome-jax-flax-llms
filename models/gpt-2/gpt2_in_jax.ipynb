{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "toc_visible": true
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [],
      "dockerImageVersionId": 30920,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "8b39517e",
      "cell_type": "markdown",
      "source": [
        "# GPT-2 JAX Implementation\n",
        "\n",
        "This notebook demonstrates how to implement and use GPT-2 with JAX for efficient text generation. It includes model loading, inference, and text generation using JAX-based tensor computations. The notebook is optimized for TPU usage but can also run on CPUs or GPUs.\n",
        "\n",
        "## Importing Required Libraries\n",
        "\n",
        "The first step is to install and import the necessary libraries. We use `tqdm` for progress bars and `datasets` for loading datasets from Hugging Face."
      ],
      "metadata": {
        "id": "8b39517e"
      }
    },
    {
      "id": "80b92b23",
      "cell_type": "markdown",
      "source": [
        "## Importing Required Libraries"
      ],
      "metadata": {
        "id": "80b92b23"
      }
    },
    {
      "id": "631acac9",
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tqdm datasets"
      ],
      "metadata": {
        "id": "631acac9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:18:59.226143Z",
          "iopub.execute_input": "2025-03-19T14:18:59.226506Z",
          "iopub.status.idle": "2025-03-19T14:19:02.890815Z",
          "shell.execute_reply.started": "2025-03-19T14:18:59.226478Z",
          "shell.execute_reply": "2025-03-19T14:19:02.889252Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8qohV72gSRbO",
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import optax\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "from typing import Any, Callable, Dict, Optional, Tuple\n",
        "from flax.training import train_state\n",
        "from functools import partial\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8qohV72gSRbO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.899829Z",
          "iopub.execute_input": "2025-03-19T14:19:02.900039Z",
          "iopub.status.idle": "2025-03-19T14:19:02.915917Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.900019Z",
          "shell.execute_reply": "2025-03-19T14:19:02.914552Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation\n",
        "We will use the `tiny_shakespeare` dataset for this example. Other datasets like `wikitext2`, `bookcorpus`, and `oscar` can also be used but may take longer to process.`\n",
        "\n"
      ],
      "metadata": {
        "id": "fLe9SKFe-Ylg"
      },
      "id": "fLe9SKFe-Ylg"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"karpathy/tiny_shakespeare\"\n",
        "\"\"\"\n",
        "Other examples may include :\n",
        "[Note that these may take longer]\n",
        "\n",
        "dataset_name = \"mindchain/wikitext2\"  # WikiText dataset\n",
        "dataset_name = \"SamuelYang/bookcorpus\"  # BookCorpus dataset\n",
        "dataset_name = \"oscar-corpus/oscar\"  # OSCAR dataset\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uhsRqRpd-VMa"
      },
      "id": "uhsRqRpd-VMa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Configuration\n",
        "The `TransformerConfig` class defines the configuration for the GPT-2 model, including hyperparameters like vocabulary size, embedding size, number of layers, and learning rate.\n",
        "\n"
      ],
      "metadata": {
        "id": "cxINIgc2-dzx"
      },
      "id": "cxINIgc2-dzx"
    },
    {
      "id": "aa3e1188",
      "cell_type": "code",
      "source": [
        "class TransformerConfig:\n",
        "    \"\"\"Configuration for the transformer model.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        block_size: int = 256,\n",
        "        n_embed: int = 384,\n",
        "        n_head: int = 6,\n",
        "        n_layer: int = 6,\n",
        "        dropout: float = 0.2,\n",
        "        learning_rate: float = 3e-4,\n",
        "        weight_decay: float = 0.1,\n",
        "        beta1: float = 0.9,\n",
        "        beta2: float = 0.95,\n",
        "        grad_clip: float = 1.0,\n",
        "        warmup_steps: int = 2000,\n",
        "        total_steps: int = 250000,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_embed = n_embed\n",
        "        self.n_head = n_head\n",
        "        self.n_layer = n_layer\n",
        "        self.dropout = dropout\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.grad_clip = grad_clip\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = total_steps"
      ],
      "metadata": {
        "id": "aa3e1188",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.916737Z",
          "iopub.execute_input": "2025-03-19T14:19:02.916983Z",
          "iopub.status.idle": "2025-03-19T14:19:02.925698Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.916960Z",
          "shell.execute_reply": "2025-03-19T14:19:02.924693Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "The GPT-2 model is implemented using JAX and Flax. The model consists of multiple transformer blocks, each containing multi-head self-attention and a feed-forward network.\n",
        "\n"
      ],
      "metadata": {
        "id": "_deGQQXf-k_3"
      },
      "id": "_deGQQXf-k_3"
    },
    {
      "id": "ggUDGuSqR7FJ",
      "cell_type": "code",
      "source": [
        "class CustomTrainState(train_state.TrainState):\n",
        "    \"\"\"Custom train state with additional fields if needed.\"\"\"\n",
        "    # Add any additional fields here if necessary\n",
        "    pass\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head causal self-attention with optimized implementation.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        config = self.config\n",
        "        assert config.n_embed % config.n_head == 0\n",
        "\n",
        "        # Key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Dense(\n",
        "            3 * config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.c_proj = nn.Dense(\n",
        "            config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "\n",
        "        # Causal mask to ensure attention only to previous tokens\n",
        "        # Instead of register_buffer, we'll make it a class variable\n",
        "        self.bias = jnp.tril(jnp.ones((config.block_size, config.block_size)))\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embed = config.n_embed\n",
        "        self.head_dim = config.n_embed // config.n_head\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        B, T, C = x.shape  # batch size, sequence length, embedding dim\n",
        "\n",
        "        # Calculate query, key, values for all heads in batch\n",
        "        q, k, v = jnp.split(self.c_attn(x), 3, axis=-1)\n",
        "\n",
        "        # Reshape to (B, T, nh, hs)\n",
        "        k = k.reshape(B, T, self.n_head, self.head_dim)\n",
        "        q = q.reshape(B, T, self.n_head, self.head_dim)\n",
        "        v = v.reshape(B, T, self.n_head, self.head_dim)\n",
        "\n",
        "        # Transpose to (B, nh, T, hs)\n",
        "        k = jnp.transpose(k, (0, 2, 1, 3))\n",
        "        q = jnp.transpose(q, (0, 2, 1, 3))\n",
        "        v = jnp.transpose(v, (0, 2, 1, 3))\n",
        "\n",
        "        # Efficient scaled dot-product attention\n",
        "        scale = jnp.sqrt(self.head_dim)\n",
        "        att = (q @ jnp.transpose(k, (0, 1, 3, 2))) / scale  # (B, nh, T, T)\n",
        "\n",
        "        # Causal mask to ensure attention only to past tokens\n",
        "        mask = self.bias[:T, :T]\n",
        "        # Use jnp.where with a mask for better TPU compatibility\n",
        "        att = jnp.where(mask == 0, jnp.full_like(att, -1e10), att)  # (B, nh, T, T)\n",
        "\n",
        "        # Softmax attention\n",
        "        att = jax.nn.softmax(att, axis=-1)\n",
        "        att = self.attn_dropout(att, deterministic=not training)\n",
        "\n",
        "        # Combine heads\n",
        "        y = att @ v  # (B, nh, T, hs)\n",
        "        y = jnp.transpose(y, (0, 2, 1, 3))  # (B, T, nh, hs)\n",
        "        y = y.reshape(B, T, C)  # (B, T, C)\n",
        "\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.c_proj(y), deterministic=not training)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"MLP with better initialization and gelu activation.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        config = self.config\n",
        "        self.c_fc = nn.Dense(\n",
        "            4 * config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "        # Use approximate=True for faster GELU on TPUs\n",
        "        self.gelu = lambda x: jax.nn.gelu(x, approximate=True)\n",
        "        self.c_proj = nn.Dense(\n",
        "            config.n_embed,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block with pre-layer normalization.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        self.ln_1 = nn.LayerNorm(epsilon=1e-5)\n",
        "        self.attn = CausalSelfAttention(self.config)\n",
        "        self.ln_2 = nn.LayerNorm(epsilon=1e-5)\n",
        "        self.mlp = MLP(self.config)\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        # Pre-layer normalization design\n",
        "        x = x + self.attn(self.ln_1(x), training=training)\n",
        "        x = x + self.mlp(self.ln_2(x), training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"GPT Language Model with improved implementation.\"\"\"\n",
        "    config: TransformerConfig\n",
        "\n",
        "    def setup(self):\n",
        "        config = self.config\n",
        "\n",
        "        # Token and position embeddings\n",
        "        self.wte = nn.Embed(\n",
        "            config.vocab_size,\n",
        "            config.n_embed,\n",
        "            embedding_init=nn.initializers.normal(stddev=0.02)\n",
        "        )\n",
        "        self.wpe = nn.Embed(\n",
        "            config.block_size,\n",
        "            config.n_embed,\n",
        "            embedding_init=nn.initializers.normal(stddev=0.02)\n",
        "        )\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = [Block(config) for _ in range(config.n_layer)]\n",
        "\n",
        "        # Final layer norm and head\n",
        "        self.ln_f = nn.LayerNorm(epsilon=1e-5)\n",
        "        self.lm_head = nn.Dense(\n",
        "            config.vocab_size,\n",
        "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
        "            bias_init=nn.initializers.zeros,\n",
        "            use_bias=False\n",
        "        )\n",
        "\n",
        "        # For weight tying\n",
        "        self.apply_weight_tying = True\n",
        "\n",
        "    def _tie_weights(self, params):\n",
        "        \"\"\"Tie embedding weights with output layer if enabled.\"\"\"\n",
        "        if not self.apply_weight_tying:\n",
        "            return params\n",
        "\n",
        "        # Clone the parameters and update lm_head kernel with embedding weights\n",
        "        new_params = params.copy()\n",
        "        new_params['lm_head']['kernel'] = new_params['wte']['embedding']\n",
        "        return new_params\n",
        "\n",
        "    def __call__(self, idx, targets=None, training=False, params=None):\n",
        "        config = self.config\n",
        "        b, t = idx.shape\n",
        "\n",
        "        # Apply weight tying if enabled (only during inference)\n",
        "        if params is not None and self.apply_weight_tying and not training:\n",
        "            params = self._tie_weights(params)\n",
        "\n",
        "        # Get token and position embeddings\n",
        "        token_emb = self.wte(idx)  # (b, t, n_embed)\n",
        "        pos = jnp.arange(0, t, dtype=jnp.int32)\n",
        "        pos_emb = self.wpe(pos)  # (t, n_embed)\n",
        "\n",
        "        # Sum embeddings and apply dropout\n",
        "        x = token_emb + pos_emb\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        # Apply final layer norm\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        logits = self.lm_head(x)  # (b, t, vocab_size)\n",
        "\n",
        "        # If targets are provided, compute loss\n",
        "        if targets is not None:\n",
        "            # Cross-entropy loss for next token prediction\n",
        "            loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "                logits.reshape(-1, config.vocab_size),\n",
        "                targets.reshape(-1)\n",
        "            ).mean()\n",
        "            return logits, loss\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ggUDGuSqR7FJ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.927068Z",
          "iopub.execute_input": "2025-03-19T14:19:02.927284Z",
          "iopub.status.idle": "2025-03-19T14:19:02.954053Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.927264Z",
          "shell.execute_reply": "2025-03-19T14:19:02.952934Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "onvhoeOfSAjX",
      "cell_type": "code",
      "source": [
        "# ---------- Data Loading ----------\n",
        "def get_batch(rng_key, data, batch_size, block_size):\n",
        "    \"\"\"Get a random batch of data with safety checks.\"\"\"\n",
        "    data_size = data.shape[0]\n",
        "\n",
        "    # Safety check: ensure data is large enough\n",
        "    if data_size <= block_size:\n",
        "        raise ValueError(f\"Data size ({data_size}) must be larger than block size ({block_size})\")\n",
        "\n",
        "    rng_key, split_key = jax.random.split(rng_key)\n",
        "\n",
        "    # Use jax.random.randint for generating random indices\n",
        "    max_start_idx = data_size - block_size - 1\n",
        "    indices = jax.random.randint(\n",
        "        split_key,\n",
        "        shape=(batch_size,),\n",
        "        minval=0,\n",
        "        maxval=max_start_idx\n",
        "    )\n",
        "\n",
        "    # TPU-optimized data loading using vectorized operations\n",
        "    idx = jnp.arange(block_size)\n",
        "    offsets = indices[:, None]\n",
        "    x_indices = offsets + idx\n",
        "    y_indices = offsets + idx + 1\n",
        "\n",
        "    x = jnp.take(data, x_indices, axis=0)\n",
        "    y = jnp.take(data, y_indices, axis=0)\n",
        "\n",
        "    return x, y, rng_key\n"
      ],
      "metadata": {
        "id": "onvhoeOfSAjX",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.955045Z",
          "iopub.execute_input": "2025-03-19T14:19:02.955266Z",
          "iopub.status.idle": "2025-03-19T14:19:02.967596Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.955244Z",
          "shell.execute_reply": "2025-03-19T14:19:02.966614Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup\n",
        "The training setup includes functions for creating the training state, preparing batches, and defining the training and evaluation steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "w9ifLnBc-wmJ"
      },
      "id": "w9ifLnBc-wmJ"
    },
    {
      "id": "3uCdC307SGXG",
      "cell_type": "code",
      "source": [
        "def create_train_state(rng_key, config):\n",
        "    \"\"\"Create initial training state.\"\"\"\n",
        "    model = GPT(config)\n",
        "\n",
        "    # Initialize model parameters with a properly shaped input\n",
        "    dummy_input = jnp.ones((8, 64), dtype=jnp.int32)  # Use batch size divisible by 8 for TPU\n",
        "    params = model.init(rng_key, dummy_input, training=False)[\"params\"]\n",
        "\n",
        "    # Create optimizer with learning rate schedule\n",
        "    lr_schedule = optax.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=config.learning_rate,\n",
        "        warmup_steps=config.warmup_steps,\n",
        "        decay_steps=config.total_steps,\n",
        "        end_value=config.learning_rate * 0.1,\n",
        "    )\n",
        "\n",
        "    # AdamW optimizer with weight decay and gradient clipping\n",
        "    optimizer = optax.chain(\n",
        "        optax.clip_by_global_norm(config.grad_clip),\n",
        "        optax.adamw(\n",
        "            learning_rate=lr_schedule,\n",
        "            b1=config.beta1,\n",
        "            b2=config.beta2,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return CustomTrainState.create(\n",
        "        apply_fn=model.apply,\n",
        "        params=params,\n",
        "        tx=optimizer\n",
        "    )\n",
        "\n",
        "# TPU-optimized training step with pmap support\n",
        "@partial(jax.pmap, axis_name='batch', static_broadcasted_argnums=(3,))\n",
        "def train_step_pmap(state, batch, rng_keys, training=True):\n",
        "    \"\"\"Single training step with parallel processing support for TPUs.\"\"\"\n",
        "    inputs, targets = batch\n",
        "\n",
        "    # Use different dropout key for each step and device\n",
        "    dropout_rng = jax.random.fold_in(\n",
        "        rng_keys,\n",
        "        state.step\n",
        "    )\n",
        "\n",
        "    def loss_fn(params):\n",
        "        logits, loss = state.apply_fn(\n",
        "            {\"params\": params},\n",
        "            inputs,\n",
        "            targets=targets,\n",
        "            training=training,\n",
        "            rngs={\"dropout\": dropout_rng}\n",
        "        )\n",
        "        return loss, logits\n",
        "\n",
        "    # Compute loss and gradients\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(state.params)\n",
        "\n",
        "    # Average gradients across replicas\n",
        "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
        "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
        "\n",
        "    # Update model parameters\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "\n",
        "    return new_state, metrics, logits\n",
        "\n",
        "\n",
        "# Standard training step for single device\n",
        "@partial(jax.jit, static_argnums=(3,))\n",
        "def train_step(state, batch, rng_key, training=True):\n",
        "    \"\"\"Single training step for single device.\"\"\"\n",
        "    inputs, targets = batch\n",
        "\n",
        "    # Use different dropout key for each step\n",
        "    dropout_rng = jax.random.fold_in(rng_key, state.step)\n",
        "\n",
        "    def loss_fn(params):\n",
        "        logits, loss = state.apply_fn(\n",
        "            {\"params\": params},\n",
        "            inputs,\n",
        "            targets=targets,\n",
        "            training=training,\n",
        "            rngs={\"dropout\": dropout_rng}\n",
        "        )\n",
        "        return loss, logits\n",
        "\n",
        "    # Compute loss and gradients\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(state.params)\n",
        "\n",
        "    # Update model parameters\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "\n",
        "    return new_state, metrics, logits\n",
        "\n",
        "\n",
        "# TPU-optimized evaluation step with pmap support\n",
        "@partial(jax.pmap, axis_name='batch', static_broadcasted_argnums=(2,))\n",
        "def eval_step_pmap(state, batch, training=False):\n",
        "    \"\"\"Evaluation step with parallel processing support for TPUs.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    logits, loss = state.apply_fn(\n",
        "        {\"params\": state.params},\n",
        "        inputs,\n",
        "        targets=targets,\n",
        "        training=training,\n",
        "    )\n",
        "\n",
        "    # Average loss across replicas\n",
        "    loss = jax.lax.pmean(loss, axis_name='batch')\n",
        "\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# Standard evaluation step for single device\n",
        "@partial(jax.jit, static_argnums=(2,))\n",
        "def eval_step(state, batch, training=False):\n",
        "    \"\"\"Evaluation step for single device.\"\"\"\n",
        "    inputs, targets = batch\n",
        "    logits, loss = state.apply_fn(\n",
        "        {\"params\": state.params},\n",
        "        inputs,\n",
        "        targets=targets,\n",
        "        training=training,\n",
        "    )\n",
        "    metrics = {\n",
        "        \"loss\": loss,\n",
        "        \"perplexity\": jnp.exp(loss),\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "3uCdC307SGXG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.968679Z",
          "iopub.execute_input": "2025-03-19T14:19:02.968924Z",
          "iopub.status.idle": "2025-03-19T14:19:02.984855Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.968903Z",
          "shell.execute_reply": "2025-03-19T14:19:02.983825Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "up4L7GuoSeXs",
      "cell_type": "code",
      "source": [
        "@partial(jax.jit, static_argnums=(2, 3, 4, 5))\n",
        "def generate_step(params, idx, block_size, temperature=1.0, top_k=40, apply_fn=None, rng_key=None):\n",
        "    \"\"\"Single generation step using top-k sampling.\"\"\"\n",
        "    # Take the last block_size tokens as context (or fewer if not enough)\n",
        "    context_size = min(idx.shape[1], block_size)\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "\n",
        "    # Get logits from the model\n",
        "    logits = apply_fn({\"params\": params}, idx_cond, training=False)\n",
        "\n",
        "    # Focus only on the last time step\n",
        "    logits = logits[:, -1, :] / temperature\n",
        "\n",
        "    # Optional top-k sampling using JAX's efficient operators\n",
        "    if top_k > 0:\n",
        "        top_k = min(top_k, logits.shape[-1])\n",
        "        topk_values, _ = jax.lax.top_k(logits, top_k)\n",
        "        threshold = topk_values[:, -1]\n",
        "        logits = jnp.where(logits < threshold[:, None], jnp.full_like(logits, -1e10), logits)\n",
        "\n",
        "    # Sample from the distribution\n",
        "    sample = jax.random.categorical(rng_key, logits, axis=-1)\n",
        "\n",
        "    # Append to the sequence\n",
        "    return jnp.concatenate([idx, sample[:, None]], axis=1)\n",
        "\n",
        "\n",
        "def generate(\n",
        "    params,\n",
        "    apply_fn,\n",
        "    prompt_idx,\n",
        "    rng_key,\n",
        "    max_new_tokens=100,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    block_size=256\n",
        "):\n",
        "    \"\"\"Generate text using the model with proper handling of long prompts.\"\"\"\n",
        "    # Handle prompt that may be longer than block_size\n",
        "    if prompt_idx.shape[1] > block_size:\n",
        "        # Only keep the last block_size tokens of the prompt\n",
        "        idx = prompt_idx[:, -block_size:]\n",
        "        print(f\"Warning: Prompt was truncated to the last {block_size} tokens due to context length limit.\")\n",
        "    else:\n",
        "        idx = prompt_idx\n",
        "\n",
        "    # Generate tokens one by one\n",
        "    for _ in range(max_new_tokens):\n",
        "        rng_key, next_key = jax.random.split(rng_key)\n",
        "        idx = generate_step(\n",
        "            params,\n",
        "            idx,\n",
        "            block_size,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            apply_fn=apply_fn,\n",
        "            rng_key=next_key\n",
        "        )\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "# ---------- TPU Initialization and Multi-device Training ----------\n",
        "def initialize_tpu():\n",
        "    \"\"\"Initialize TPU system.\"\"\"\n",
        "    # Check if running on TPU\n",
        "    if 'tpu' in jax.devices()[0].platform:\n",
        "        print(f\"Running on {jax.device_count()} TPU devices\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Not running on TPU\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def replicate_state_on_devices(state):\n",
        "    \"\"\"Replicate state across all TPU devices.\"\"\"\n",
        "    # Broadcast the state to all devices\n",
        "    state = jax.device_put_replicated(state, jax.local_devices())\n",
        "    return state\n",
        "\n",
        "\n",
        "def prepare_batch_for_devices(batch, num_devices):\n",
        "    \"\"\"Prepare batch for multiple devices by reshaping.\"\"\"\n",
        "    x, y = batch\n",
        "\n",
        "    # Get total batch size and calculate per-device batch size\n",
        "    total_batch_size = x.shape[0]\n",
        "    per_device_batch_size = total_batch_size // num_devices\n",
        "\n",
        "    # Reshape to (num_devices, per_device_batch_size, ...)\n",
        "    x = x.reshape((num_devices, per_device_batch_size) + x.shape[1:])\n",
        "    y = y.reshape((num_devices, per_device_batch_size) + y.shape[1:])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def create_device_rng_keys(rng_key, num_devices):\n",
        "    \"\"\"Create separate RNG keys for each device.\"\"\"\n",
        "    # Split the main key into num_devices keys\n",
        "    return jax.random.split(rng_key, num_devices)\n",
        "\n",
        "\n",
        "def step_device_rng_keys(rng_keys):\n",
        "    \"\"\"Update RNG keys for each device independently.\"\"\"\n",
        "    # Split each device's key to get a new key for each device\n",
        "    new_keys = jax.vmap(jax.random.split)(rng_keys)\n",
        "    # Each split returns 2 keys, keep the first one for each device\n",
        "    return new_keys[:, 0]"
      ],
      "metadata": {
        "id": "up4L7GuoSeXs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:02.985847Z",
          "iopub.execute_input": "2025-03-19T14:19:02.986052Z",
          "iopub.status.idle": "2025-03-19T14:19:03.001007Z",
          "shell.execute_reply.started": "2025-03-19T14:19:02.986032Z",
          "shell.execute_reply": "2025-03-19T14:19:02.999819Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "mISHmd0eoap9",
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset_name, split=\"train\"):\n",
        "    \"\"\"\n",
        "    Prepare a Hugging Face dataset for training.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: str, name of the dataset on Hugging Face\n",
        "        split: str, which split to use (default: \"train\")\n",
        "\n",
        "    Returns:\n",
        "        train_data: encoded training data\n",
        "        val_data: encoded validation data\n",
        "        encode_fn: function to encode text\n",
        "        decode_fn: function to decode indices\n",
        "        vocab_size: size of vocabulary\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    dataset = load_dataset(dataset_name)\n",
        "\n",
        "    # Get text from dataset\n",
        "    if \"text\" in dataset[split].features:\n",
        "        text_key = \"text\"\n",
        "    else:\n",
        "        # Try to find a text field\n",
        "        text_fields = [k for k, v in dataset[split].features.items()\n",
        "                      if v.dtype == 'string']\n",
        "        if text_fields:\n",
        "            text_key = text_fields[0]\n",
        "        else:\n",
        "            raise ValueError(\"Could not find text field in dataset\")\n",
        "\n",
        "    # Combine all texts\n",
        "    text = \"\\n\".join(dataset[split][text_key])\n",
        "\n",
        "    # Create vocabulary\n",
        "    chars = sorted(list(set(text)))\n",
        "    vocab_size = len(chars)\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "    # Create encode/decode functions\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "    # Encode full text\n",
        "    data = jnp.array(encode(text))\n",
        "\n",
        "    # Split into train/val\n",
        "    n = int(0.9 * len(data))\n",
        "    train_data = data[:n]\n",
        "    val_data = data[n:]\n",
        "\n",
        "    return train_data, val_data, encode, decode, vocab_size"
      ],
      "metadata": {
        "id": "mISHmd0eoap9",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:03.002125Z",
          "iopub.execute_input": "2025-03-19T14:19:03.002362Z",
          "iopub.status.idle": "2025-03-19T14:19:03.014291Z",
          "shell.execute_reply.started": "2025-03-19T14:19:03.002338Z",
          "shell.execute_reply": "2025-03-19T14:19:03.013316Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop\n",
        "The training loop runs for a specified number of steps, printing the loss and perplexity periodically.\n",
        "\n"
      ],
      "metadata": {
        "id": "tuwPnU55-1_e"
      },
      "id": "tuwPnU55-1_e"
    },
    {
      "id": "vXDYFJ93Shje",
      "cell_type": "code",
      "source": [
        "use_tpu = initialize_tpu()\n",
        "num_devices = jax.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXDYFJ93Shje",
        "outputId": "244e0f27-f687-4d3a-e6ea-62287d302117",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:03.015333Z",
          "iopub.execute_input": "2025-03-19T14:19:03.015544Z",
          "iopub.status.idle": "2025-03-19T14:19:03.026333Z",
          "shell.execute_reply.started": "2025-03-19T14:19:03.015523Z",
          "shell.execute_reply": "2025-03-19T14:19:03.025286Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Running on 8 TPU devices\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "0DxBfEE_c23p",
      "cell_type": "code",
      "source": [
        "# Load dataset from Hugging Face\n",
        "train_data, val_data, encode, decode, vocab_size = prepare_dataset(dataset_name)\n",
        "# Calculate reasonable total_steps based on dataset size\n",
        "data_size = len(train_data)\n",
        "batch_size = min(64, data_size // 10)  # Cap batch size at 64\n",
        "if use_tpu:\n",
        "    # Scale batch size responsibly\n",
        "    devices_to_use = min(num_devices, 8)  # Cap device usage if too many\n",
        "    batch_size = batch_size * devices_to_use\n",
        "\n",
        "# Estimate total steps based on dataset size and batch size\n",
        "# Aim for ~100 epochs for small datasets, fewer for larger ones\n",
        "epochs = max(10, min(100, 1000000 // data_size))\n",
        "total_steps = (data_size * epochs) // batch_size\n",
        "\n",
        "config = TransformerConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    block_size=min(256, data_size // 2),  # Ensure block_size isn't too large\n",
        "    n_embed=384,\n",
        "    n_head=6,\n",
        "    n_layer=6,\n",
        "    dropout=0.2,\n",
        "    learning_rate=3e-4,\n",
        "    total_steps=total_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "0DxBfEE_c23p",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:03.027423Z",
          "iopub.execute_input": "2025-03-19T14:19:03.027639Z",
          "iopub.status.idle": "2025-03-19T14:19:07.576740Z",
          "shell.execute_reply.started": "2025-03-19T14:19:03.027617Z",
          "shell.execute_reply": "2025-03-19T14:19:07.575261Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "taBtpmR_c6hP",
      "cell_type": "code",
      "source": [
        "rng_key = jax.random.PRNGKey(42)\n",
        "rng_key, init_key = jax.random.split(rng_key)\n",
        "train_state = create_train_state(init_key, config)\n",
        "\n",
        "# For TPU, replicate the state across devices\n",
        "if use_tpu:\n",
        "    train_state = replicate_state_on_devices(train_state)\n",
        "    # Create separate rng keys for each device\n",
        "    device_rng_keys = create_device_rng_keys(rng_key, num_devices)"
      ],
      "metadata": {
        "id": "taBtpmR_c6hP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:07.577642Z",
          "iopub.execute_input": "2025-03-19T14:19:07.577906Z",
          "iopub.status.idle": "2025-03-19T14:19:15.519529Z",
          "shell.execute_reply.started": "2025-03-19T14:19:07.577882Z",
          "shell.execute_reply": "2025-03-19T14:19:15.518251Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2yMh_joHdA0n",
      "cell_type": "code",
      "source": [
        "print(f\"Training for {total_steps} steps with batch size {batch_size}\")\n",
        "steps_to_run = min(5000, total_steps)  # Cap at 5000 steps for example\n",
        "\n",
        "for step in range(steps_to_run):\n",
        "    try:\n",
        "        # Get batch\n",
        "        if use_tpu:\n",
        "            # For TPU, create a batch for each device\n",
        "            x, y, rng_key = get_batch(rng_key, train_data, batch_size, config.block_size)\n",
        "            x, y = prepare_batch_for_devices((x, y), num_devices)\n",
        "\n",
        "            # Update device RNG keys correctly\n",
        "            device_rng_keys = step_device_rng_keys(device_rng_keys)\n",
        "\n",
        "            # Train step with parallel map\n",
        "            train_state, metrics, _ = train_step_pmap(train_state, (x, y), device_rng_keys, True)\n",
        "\n",
        "            # Extract metrics from first device (all are same due to pmean)\n",
        "            metrics = {k: v[0] for k, v in metrics.items()}\n",
        "        else:\n",
        "            # Standard single-device training\n",
        "            x, y, rng_key = get_batch(rng_key, train_data, batch_size, config.block_size)\n",
        "            train_state, metrics, _ = train_step(train_state, (x, y), rng_key)\n",
        "\n",
        "        # Print metrics occasionally\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}: loss = {metrics['loss']:.4f}, perplexity = {metrics['perplexity']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training step {step}: {e}\")\n",
        "        break"
      ],
      "metadata": {
        "id": "2yMh_joHdA0n",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:19:15.520309Z",
          "iopub.execute_input": "2025-03-19T14:19:15.520561Z",
          "iopub.status.idle": "2025-03-19T14:26:57.299546Z",
          "shell.execute_reply.started": "2025-03-19T14:19:15.520536Z",
          "shell.execute_reply": "2025-03-19T14:26:57.298503Z"
        },
        "outputId": "5ca995cb-634a-4596-d111-340f24c8f9d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training for 17645 steps with batch size 512\nStep 0: loss = 4.2108, perplexity = 67.4134\nStep 100: loss = 3.0069, perplexity = 20.2245\nStep 200: loss = 2.5834, perplexity = 13.2424\nStep 300: loss = 2.4690, perplexity = 11.8106\nStep 400: loss = 2.4072, perplexity = 11.1032\nStep 500: loss = 2.3299, perplexity = 10.2771\nStep 600: loss = 2.2287, perplexity = 9.2878\nStep 700: loss = 2.1027, perplexity = 8.1880\nStep 800: loss = 1.9693, perplexity = 7.1653\nStep 900: loss = 1.8649, perplexity = 6.4554\nStep 1000: loss = 1.7600, perplexity = 5.8126\nStep 1100: loss = 1.6540, perplexity = 5.2277\nStep 1200: loss = 1.5906, perplexity = 4.9069\nStep 1300: loss = 1.4975, perplexity = 4.4703\nStep 1400: loss = 1.4422, perplexity = 4.2298\nStep 1500: loss = 1.4045, perplexity = 4.0734\nStep 1600: loss = 1.3508, perplexity = 3.8606\nStep 1700: loss = 1.3106, perplexity = 3.7085\nStep 1800: loss = 1.2755, perplexity = 3.5806\nStep 1900: loss = 1.2413, perplexity = 3.4600\nStep 2000: loss = 1.2150, perplexity = 3.3703\nStep 2100: loss = 1.1792, perplexity = 3.2517\nStep 2200: loss = 1.1420, perplexity = 3.1329\nStep 2300: loss = 1.1171, perplexity = 3.0559\nStep 2400: loss = 1.0915, perplexity = 2.9787\nStep 2500: loss = 1.0589, perplexity = 2.8832\nStep 2600: loss = 1.0247, perplexity = 2.7864\nStep 2700: loss = 0.9888, perplexity = 2.6879\nStep 2800: loss = 0.9567, perplexity = 2.6031\nStep 2900: loss = 0.9326, perplexity = 2.5411\nStep 3000: loss = 0.8913, perplexity = 2.4382\nStep 3100: loss = 0.8684, perplexity = 2.3831\nStep 3200: loss = 0.8397, perplexity = 2.3157\nStep 3300: loss = 0.8079, perplexity = 2.2432\nStep 3400: loss = 0.7800, perplexity = 2.1814\nStep 3500: loss = 0.7584, perplexity = 2.1348\nStep 3600: loss = 0.7305, perplexity = 2.0761\nStep 3700: loss = 0.7033, perplexity = 2.0204\nStep 3800: loss = 0.6766, perplexity = 1.9673\nStep 3900: loss = 0.6552, perplexity = 1.9256\nStep 4000: loss = 0.6415, perplexity = 1.8993\nStep 4100: loss = 0.6137, perplexity = 1.8473\nStep 4200: loss = 0.5969, perplexity = 1.8165\nStep 4300: loss = 0.5782, perplexity = 1.7828\nStep 4400: loss = 0.5554, perplexity = 1.7426\nStep 4500: loss = 0.5374, perplexity = 1.7115\nStep 4600: loss = 0.5230, perplexity = 1.6870\nStep 4700: loss = 0.5072, perplexity = 1.6606\nStep 4800: loss = 0.4959, perplexity = 1.6419\nStep 4900: loss = 0.4782, perplexity = 1.6132\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation\n",
        "After training, we can generate text using the trained model. The generation process uses top-k sampling to produce coherent text.\n",
        "\n"
      ],
      "metadata": {
        "id": "7clu5fY4-6et"
      },
      "id": "7clu5fY4-6et"
    },
    {
      "id": "f9KzvJ3adCpG",
      "cell_type": "code",
      "source": [
        "try:\n",
        "    prompt = \"ROMEO:\"\n",
        "    print(f\"Generating text from prompt: '{prompt}'\")\n",
        "    prompt_idx = jnp.array([encode(prompt)])\n",
        "\n",
        "    # If using TPU, get params from first device\n",
        "    if use_tpu:\n",
        "        params = jax.tree_util.tree_map(lambda x: x[0], train_state.params)\n",
        "        apply_fn = train_state.apply_fn\n",
        "    else:\n",
        "        params = train_state.params\n",
        "        apply_fn = train_state.apply_fn\n",
        "\n",
        "    rng_key, gen_key = jax.random.split(rng_key)\n",
        "    generated_idx = generate(\n",
        "        params,\n",
        "        apply_fn,\n",
        "        prompt_idx,\n",
        "        gen_key,\n",
        "        max_new_tokens=500,\n",
        "        temperature=0.8,\n",
        "        top_k=40,\n",
        "        block_size=config.block_size\n",
        "    )\n",
        "\n",
        "    generated_text = decode(generated_idx[0].tolist())\n",
        "    print(generated_text)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during text generation: {e}\")\n"
      ],
      "metadata": {
        "id": "f9KzvJ3adCpG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-19T14:26:57.301066Z",
          "iopub.execute_input": "2025-03-19T14:26:57.301435Z",
          "iopub.status.idle": "2025-03-19T14:39:18.888366Z",
          "shell.execute_reply.started": "2025-03-19T14:26:57.301384Z",
          "shell.execute_reply": "2025-03-19T14:39:18.887372Z"
        },
        "outputId": "cf0bb2a6-2402-41ca-d302-d50203d49906"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Generating text from prompt: 'ROMEO:'\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_10/2712560032.py:8: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n  params = jax.tree_map(lambda x: x[0], train_state.params)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ROMEO:\nI am a poor fellow, so still my purpose.\n\nNurse:\nReady not out of my sin George to three,\nHe's many of your mother was to grant my chose.\n\nJULIET:\nI will confess thee; and if thou never so die,\nBetter by any in the house of Lancaster.\n\nROMEO:\nAnd so I twenty years. O, here comes my nurse,\nAnd I'll signify to thy tongue my banishment.\n\nJULIET:\nI cannot now get thee hence; for I took him.\n\nNurse:\nLord, his own is odd, when it doth begue;\nGive me the heavens thy heart.\n\nJULIET:\nO holy friar, and g\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}