# ğŸ“ GPT-2 in JAX/Flax

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dhyaneesh/awesome-jax-flax-llms/blob/main/gpt2_in_jax.ipynb)

This folder contains a JAX/Flax implementation of the **GPT-2** language model. As part of the larger **awesome-jax-flax-llms** project, this implementation demonstrates how to efficiently train and run transformer-based models on **TPUs and GPUs** using JAX.

## ğŸš€ Overview

The `gpt2_in_jax.ipynb` notebook provides a clean and modular implementation of **GPT-2** using **JAX & Flax**, leveraging JAXâ€™s **XLA compilation** and **Optax optimizers** for accelerated training. This implementation is designed for:

- Efficient **autoregressive text generation**.
- Scalable training with **JAXâ€™s parallelization features**.
- Fine-tuning on **small to medium-scale datasets**.

## ğŸ¤– Understanding GPT-2

GPT-2 (Generative Pre-trained Transformer 2) is an **autoregressive language model** that generates text by predicting the next token in a sequence. It is built using a stack of **Transformer decoder layers**, utilizing **self-attention mechanisms** to capture long-range dependencies. The key components of GPT-2 include:

- **Multi-head Self-Attention**: Enables the model to attend to different parts of the input sequence simultaneously.
- **Feedforward Neural Networks**: Processes the attended information to make predictions.
- **Positional Encoding**: Incorporates word order information since transformers lack built-in sequence awareness.
- **Layer Normalization & Dropout**: Improves stability and prevents overfitting.

---

![GPT-2 Architecture](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*YZTqlV51QyhX6VL9AV31eQ.png)

---

## ğŸ›  Features

- âœ… **Pure JAX/Flax implementation** of GPT-2.
- âœ… **Optimized for TPUs & GPUs** using JAXâ€™s Just-In-Time (JIT) compilation.
- âœ… **Optax-based training** for efficient optimization.
- âœ… **Flexible model configuration**, allowing easy scaling.
- âœ… **Minimal dependencies**, making it lightweight & easy to extend.

## ğŸ“Œ Notebook Details

The notebook includes:

- ğŸ“– **Dataset Preparation**: Downloads and processes the *tinyshakespeare* dataset from GitHub.
- ğŸ— **Model Definition**: GPT-2 architecture built using `Flax.linen`.
- ğŸ¯ **Training Pipeline**: Implements loss computation, backpropagation, and optimization using `Optax`.
- ğŸ **Inference & Generation**: Generates text samples efficiently with autoregressive decoding.
- ğŸ“Š **Performance Evaluation**: Tracks training progress and visualizes loss curves.

## ğŸ— Setup & Usage

### **1ï¸âƒ£ Install Dependencies**

Ensure you have JAX and Flax installed. Run the following:

```bash
pip install jax flax optax datasets transformers
```

### **2ï¸âƒ£ Run the Notebook**

Execute the `gpt2_in_jax.ipynb` notebook step by step in **Google Colab (with TPU runtime)** or a local Jupyter environment with GPU support.

### **3ï¸âƒ£ Fine-tune GPT-2** (Optional)

To fine-tune on custom datasets, modify the training loop and load your dataset using **Hugging Face Datasets** or a custom data pipeline.

## ğŸ“– Next Steps

- ğŸ”„ **Enable longer context training** for improved coherence.
- ğŸª› **Finetuning** the model using Hugging Face Datasets or a custom data pipeline.
- âš¡ Optimize inference with XLA caching.
- ğŸ“š **Experiment with different tokenization methods**.

## ğŸ“œ License

This project is licensed under the **GPL-3.0** license. See the [LICENSE](../LICENSE) file for details.

---

ğŸ’¡ Contributions & feedback are welcome! ğŸš€

