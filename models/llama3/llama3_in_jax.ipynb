{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:01:48.723948Z","iopub.execute_input":"2025-03-21T19:01:48.724330Z","iopub.status.idle":"2025-03-21T19:01:52.588421Z","shell.execute_reply.started":"2025-03-21T19:01:48.724302Z","shell.execute_reply":"2025-03-21T19:01:52.586545Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport math\nimport pickle\nimport jax\nimport jax.numpy as jnp\nfrom jax import random, lax, vmap\nimport flax.linen as nn\nimport optax\nfrom types import SimpleNamespace\nfrom functools import partial\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom tokenizers import SentencePieceUnigramTokenizer\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\nfrom flax.training import train_state, orbax_utils\nimport orbax.checkpoint as ocp","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:13:19.771071Z","iopub.execute_input":"2025-03-21T19:13:19.771656Z","iopub.status.idle":"2025-03-21T19:13:19.777634Z","shell.execute_reply.started":"2025-03-21T19:13:19.771619Z","shell.execute_reply":"2025-03-21T19:13:19.776531Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"os.environ['JAX_PLATFORM_NAME'] = 'tpu'\nos.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\nprint(\"JAX devices:\", len(jax.devices()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:02:58.278053Z","iopub.execute_input":"2025-03-21T19:02:58.278477Z","iopub.status.idle":"2025-03-21T19:02:58.283405Z","shell.execute_reply.started":"2025-03-21T19:02:58.278440Z","shell.execute_reply":"2025-03-21T19:02:58.282212Z"}},"outputs":[{"name":"stdout","text":"JAX devices: 8\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class LLaMAConfig:\n    \"\"\"Configuration for LLaMA model and dataset preparation\"\"\"\n    # Model architecture settings\n    vocab_size: int = 32000\n    dim: int = 512  # Hidden dimension\n    n_layers: int = 8  # Number of transformer layers\n    n_heads: int = 8  # Number of attention heads\n    n_kv_heads: int = 4  # Number of key/value heads (for grouped-query attention)\n    max_seq_len: int = 2048  # Maximum sequence length\n    dropout_rate: float = 0.0  # Dropout rate\n    # RoPE settings\n    rope_theta: float = 10000.0  # Base for rotary embeddings\n    \n    # Training settings\n    batch_size: int = 16\n    learning_rate: float = 3e-4\n    weight_decay: float = 0.1\n    warmup_steps: int = 1000\n    max_steps: int = 100000\n    \n    # Generation settings\n    temperature: float = 0.8\n    top_k: int = 40\n    top_p: float = 0.95\n    \n    # Dataset settings\n    dataset_name: str = \"wikitext\"  # Default dataset\n    dataset_config: str = \"wikitext-2-raw-v1\"  # Dataset configuration name\n    dataset_path: str = None  # Path to local dataset\n    split: str = \"train\"  # Dataset split to use\n    text_column: str = None  # Column to use for text (auto-detected if None)\n    \n    # Tokenizer settings\n    tokenizer_path: str = \"llama_tokenizer.json\"  # Path to save/load tokenizer\n    tokenizer_save_path: str = None  # Optional different path to save tokenizer\n    use_existing_tokenizer: bool = False  # Whether to use existing tokenizer\n    tokenizer_sample_size: int = 10000  # Number of samples to use for tokenizer training\n    special_tokens: list = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]  # List of special tokens (defaults to [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"])\n    \n    # Processing options\n    format_type: str = \"flat\"  # Dataset format: \"flat\", \"chunked\", or default\n    chunk_size: int = 1024  # Size of chunks for \"chunked\" format\n    chunk_overlap: int = 0  # Overlap between chunks\n    add_eos_between_examples: bool = True  # Whether to add EOS token between examples\n    add_eos_between_segments: bool = False  # Whether to add EOS token between segments\n    keep_text_column: bool = False  # Whether to keep original text column\n\n    def __post_init__(self):\n        \"\"\"Initialize defaults for None values\"\"\"\n        if self.special_tokens is None:\n            self.special_tokens = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n        if self.tokenizer_save_path is None:\n            self.tokenizer_save_path = self.tokenizer_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:22:11.306585Z","iopub.execute_input":"2025-03-21T19:22:11.306930Z","iopub.status.idle":"2025-03-21T19:22:11.315389Z","shell.execute_reply.started":"2025-03-21T19:22:11.306906Z","shell.execute_reply":"2025-03-21T19:22:11.314185Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class RMSNorm(nn.Module):\n    \"\"\"Root Mean Square Layer Normalization\"\"\"\n    dim: int\n    eps: float = 1e-5\n\n    @nn.compact\n    def __call__(self, x):\n        weight = self.param('weight', nn.initializers.ones, (self.dim,))\n        variance = jnp.mean(jnp.square(x), axis=-1, keepdims=True)\n        x = x * jnp.reciprocal(jnp.sqrt(variance + self.eps))\n        return x * weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:04:57.437522Z","iopub.execute_input":"2025-03-21T19:04:57.437902Z","iopub.status.idle":"2025-03-21T19:04:57.445333Z","shell.execute_reply.started":"2025-03-21T19:04:57.437875Z","shell.execute_reply":"2025-03-21T19:04:57.443964Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Rotary Position Embeddings\ndef precompute_freqs_cis(dim: int, max_seq_len: int, theta: float = 10000.0):\n    \"\"\"Precompute the frequency tensor for complex exponentials (rotary embeddings).\"\"\"\n    # Compute the frequencies for each feature dimension\n    freqs = 1.0 / (theta ** (jnp.arange(0, dim // 2, dtype=jnp.float32) / dim))\n    t = jnp.arange(max_seq_len, dtype=jnp.float32)\n    # Create the frequency matrix by outer product\n    freqs = jnp.outer(t, freqs)\n    # Convert to complex exponentials\n    return jnp.complex64(jnp.exp(1j * freqs))\n\ndef apply_rotary_emb(xq, xk, freqs_cis):\n    \"\"\"Apply rotary embeddings to the query and key tensors.\"\"\"\n    # Reshape inputs to isolate the last dimension into pairs for complex multiplication\n    xq_r, xk_r = jnp.reshape(xq, (*xq.shape[:-1], -1, 2)), jnp.reshape(xk, (*xk.shape[:-1], -1, 2))\n\n    # Convert to complex numbers\n    xq_complex = jnp.complex64(xq_r[..., 0] + 1j * xq_r[..., 1])\n    xk_complex = jnp.complex64(xk_r[..., 0] + 1j * xk_r[..., 1])\n\n    # Reshape frequency cis for broadcasting\n    freqs_cis = jnp.reshape(freqs_cis, (1, freqs_cis.shape[0], 1, freqs_cis.shape[1]))\n\n    # Apply rotation through complex multiplication\n    xq_out = xq_complex * freqs_cis\n    xk_out = xk_complex * freqs_cis\n\n    # Convert back to real tensor and reshape\n    xq = jnp.stack([jnp.real(xq_out), jnp.imag(xq_out)], axis=-1).reshape(xq.shape)\n    xk = jnp.stack([jnp.real(xk_out), jnp.imag(xk_out)], axis=-1).reshape(xk.shape)\n\n    return xq, xk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:05:15.909510Z","iopub.execute_input":"2025-03-21T19:05:15.909886Z","iopub.status.idle":"2025-03-21T19:05:15.919239Z","shell.execute_reply.started":"2025-03-21T19:05:15.909858Z","shell.execute_reply":"2025-03-21T19:05:15.917799Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"@partial(jax.jit)\ndef flash_attention(q, k, v, mask=None, scale=None):\n    \"\"\"\n    Optimized implementation of attention mechanism using JAX primitives\n    for better compiler optimization and memory efficiency.\n    \"\"\"\n    batch_size, num_heads, seq_len, head_dim = q.shape\n\n    # Compute scale if not provided\n    if scale is None:\n        scale = 1.0 / jnp.sqrt(head_dim)\n\n    # Compute attention scores with fused operation\n    # Fuse transpose and matmul for better compiler optimization\n    scores = jnp.einsum('bhid,bhjd->bhij', q, k) * scale\n\n    # Apply causal mask if provided\n    if mask is not None:\n        scores = scores + mask\n\n    # Stabilize softmax by subtracting max value\n    # This prevents overflow and allows for better precision\n    scores_max = jnp.max(scores, axis=-1, keepdims=True)\n    scores = scores - lax.stop_gradient(scores_max)\n\n    # Apply softmax with higher precision\n    attn_weights = jnp.exp(scores)\n    attn_weights = attn_weights / jnp.sum(attn_weights, axis=-1, keepdims=True)\n\n    # Compute attention output with fused operation\n    output = jnp.einsum('bhij,bhjd->bhid', attn_weights, v)\n\n    return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:05:27.641206Z","iopub.execute_input":"2025-03-21T19:05:27.641590Z","iopub.status.idle":"2025-03-21T19:05:27.649394Z","shell.execute_reply.started":"2025-03-21T19:05:27.641561Z","shell.execute_reply":"2025-03-21T19:05:27.648064Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def swiglu(x, w1, w2, w3):\n    \"\"\"SwiGLU activation function using Flax modules\"\"\"\n    return w2(jax.nn.silu(w3(x)) * w1(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:05:37.615936Z","iopub.execute_input":"2025-03-21T19:05:37.616315Z","iopub.status.idle":"2025-03-21T19:05:37.620521Z","shell.execute_reply.started":"2025-03-21T19:05:37.616288Z","shell.execute_reply":"2025-03-21T19:05:37.619556Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class LLaMACausalSelfAttention(nn.Module):\n    \"\"\"Multi-head causal self-attention with support for grouped-query attention\"\"\"\n    config: LLaMAConfig\n\n    def setup(self):\n        config = self.config\n        dim = config.dim\n        n_heads = config.n_heads\n        n_kv_heads = config.n_kv_heads\n        head_dim = dim // n_heads\n\n        # QKV projections\n        self.wq = nn.Dense(n_heads * head_dim,\n                          kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n        self.wk = nn.Dense(n_kv_heads * head_dim,\n                          kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n        self.wv = nn.Dense(n_kv_heads * head_dim,\n                          kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n\n        # Output projection\n        self.wo = nn.Dense(dim,\n                          kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n\n        # QK normalization for improved stability\n        self.q_norm = RMSNorm(head_dim)\n        self.k_norm = RMSNorm(head_dim)\n\n    def __call__(self, x, freqs_cis, mask=None, deterministic=True):\n        B, T, C = x.shape\n        config = self.config\n        n_heads = config.n_heads\n        n_kv_heads = config.n_kv_heads\n        head_dim = C // n_heads\n\n        # Linear projections\n        q = self.wq(x).reshape(B, T, n_heads, head_dim)\n        k = self.wk(x).reshape(B, T, n_kv_heads, head_dim)\n        v = self.wv(x).reshape(B, T, n_kv_heads, head_dim)\n\n        # Apply QK normalization\n        q = jnp.swapaxes(self.q_norm(jnp.swapaxes(q, 1, 2)), 1, 2)\n        k = jnp.swapaxes(self.k_norm(jnp.swapaxes(k, 1, 2)), 1, 2)\n\n        # Apply rotary embeddings\n        q, k = apply_rotary_emb(q, k, freqs_cis[:T])\n\n        # Repeat k and v heads if n_heads > n_kv_heads (grouped-query attention)\n        if n_heads > n_kv_heads:\n            k = jnp.repeat(k, n_heads // n_kv_heads, axis=2)\n            v = jnp.repeat(v, n_heads // n_kv_heads, axis=2)\n\n        # Transpose tensors for attention computation (B, H, T, D)\n        q, k, v = map(lambda x: jnp.swapaxes(x, 1, 2), (q, k, v))\n\n        # Use flash attention (conceptually)\n        output = flash_attention(q, k, v, mask)\n\n        # Transpose output and project back to full dimension\n        output = jnp.swapaxes(output, 1, 2).reshape(B, T, -1)\n        output = self.wo(output)\n\n        return output\n\n# LLaMA MLP Module\nclass LLaMAMLP(nn.Module):\n    \"\"\"Feed-forward network with SwiGLU activation\"\"\"\n    config: LLaMAConfig\n\n    def setup(self):\n        dim = self.config.dim\n        hidden_dim = 4 * dim  # 4x expansion\n\n        # Linear projections\n        self.w1 = nn.Dense(hidden_dim,\n                         kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n        self.w2 = nn.Dense(dim,\n                         kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n        self.w3 = nn.Dense(hidden_dim,\n                         kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n\n    def __call__(self, x):\n        return swiglu(x, self.w1, self.w2, self.w3)\n\n# LLaMA Transformer Block\nclass LLaMABlock(nn.Module):\n    \"\"\"LLaMA transformer block\"\"\"\n    config: LLaMAConfig\n\n    def setup(self):\n        self.attention_norm = RMSNorm(self.config.dim)\n        self.attention = LLaMACausalSelfAttention(self.config)\n        self.ffn_norm = RMSNorm(self.config.dim)\n        self.ffn = LLaMAMLP(self.config)\n        self.dropout = nn.Dropout(self.config.dropout_rate)\n\n    def __call__(self, x, freqs_cis, mask=None, deterministic=True):\n        # Pre-norm for attention\n        h = x + self.dropout(\n            self.attention(self.attention_norm(x), freqs_cis, mask, deterministic),\n            deterministic=deterministic\n        )\n\n        # Pre-norm for FFN\n        out = h + self.dropout(\n            self.ffn(self.ffn_norm(h)),\n            deterministic=deterministic\n        )\n\n        return out\n\n# Full LLaMA Model\nclass LLaMA3(nn.Module):\n    \"\"\"LLaMA language model\"\"\"\n    config: LLaMAConfig\n\n    def setup(self):\n        config = self.config\n\n        # Token embeddings\n        self.token_embedding = nn.Embed(\n            config.vocab_size,\n            config.dim,\n            embedding_init=nn.initializers.normal(stddev=0.02)\n        )\n\n        # Transformer blocks\n        self.blocks = [LLaMABlock(config) for _ in range(config.n_layers)]\n\n        # Final layer norm\n        self.norm_f = RMSNorm(config.dim)\n\n        # Output projection (tied with embeddings)\n        self.lm_head = nn.Dense(\n            config.vocab_size,\n            kernel_init=nn.initializers.normal(stddev=0.02),\n            use_bias=False\n        )\n\n        # Pre-compute rotary embeddings\n        self.freqs_cis = precompute_freqs_cis(\n            config.dim // config.n_heads,\n            config.max_seq_len,\n            config.rope_theta\n        )\n\n    def __call__(self, input_ids, deterministic=True):\n        B, T = input_ids.shape\n\n        # Create causal attention mask\n        mask = jnp.tril(\n            jnp.ones((self.config.max_seq_len, self.config.max_seq_len))\n        )\n        mask = jnp.where(mask == 0, jnp.finfo(jnp.float32).min, 0.0)\n        mask = mask[None, None, :T, :T]\n\n        # Get embeddings\n        h = self.token_embedding(input_ids)\n\n        # Apply transformer blocks\n        for block in self.blocks:\n            h = block(h, self.freqs_cis, mask, deterministic)\n\n        # Apply final normalization\n        h = self.norm_f(h)\n\n        # Get logits\n        logits = self.lm_head(h)\n\n        return logits\n\n    def generate(self, input_ids, max_new_tokens, rng_key, temperature=0.8, top_k=40, top_p=0.95):\n        \"\"\"Generate text using the model\"\"\"\n        B, T = input_ids.shape\n\n        # Create initial output array\n        output = input_ids\n\n        # Generate tokens\n        for i in range(max_new_tokens):\n            # Keep the context within max sequence length\n            curr_input = output[:, -self.config.max_seq_len:]\n\n            # Get logits for the next token\n            logits = self(curr_input, deterministic=True)[:, -1, :]\n\n            # Apply temperature\n            logits = logits / temperature\n\n            # Apply top-k filtering\n            if top_k > 0:\n                top_k_v, top_k_i = jax.lax.top_k(logits, top_k)\n                indices_to_remove = jnp.broadcast_to(\n                    jnp.arange(logits.shape[-1]) < top_k_i[:, -1:],\n                    logits.shape\n                )\n                logits = jnp.where(indices_to_remove, logits, jnp.finfo(jnp.float32).min)\n\n            # Apply top-p (nucleus) filtering\n            if top_p < 1.0:\n                sorted_indices = jnp.argsort(logits, axis=-1)[:, ::-1]  # Sort indices in descending order\n                sorted_logits = jnp.take_along_axis(logits, sorted_indices, axis=-1)  # Get sorted values\n\n                cumulative_probs = jnp.cumsum(jax.nn.softmax(sorted_logits, axis=-1), axis=-1)\n\n                # Remove tokens with cumulative probability above the threshold\n                sorted_indices_to_remove = cumulative_probs > top_p\n                # Shift the indices to the right to keep the first token above threshold\n                sorted_indices_to_remove = jnp.roll(sorted_indices_to_remove, 1, axis=1)\n                sorted_indices_to_remove = sorted_indices_to_remove.at[:, 0].set(False)\n\n                # Scatter sorted tensors to original indexing\n                indices_to_remove = jnp.zeros_like(logits, dtype=bool)\n                indices_to_remove = indices_to_remove.at[jnp.arange(B)[:, None], sorted_indices].set(sorted_indices_to_remove)\n                logits = jnp.where(indices_to_remove, jnp.finfo(jnp.float32).min, logits)\n\n            # Sample from the filtered distribution\n            rng_key, sample_key = random.split(rng_key)\n            next_token = random.categorical(sample_key, logits, shape=(B,))\n\n            # Append the sampled token to the sequence\n            output = jnp.concatenate([output, next_token[:, None]], axis=1)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:05:59.851210Z","iopub.execute_input":"2025-03-21T19:05:59.851560Z","iopub.status.idle":"2025-03-21T19:05:59.885298Z","shell.execute_reply.started":"2025-03-21T19:05:59.851534Z","shell.execute_reply":"2025-03-21T19:05:59.883854Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Create initial training state with Flax TrainState\ndef create_train_state(model, config, rng_key):\n    \"\"\"Create initial training state.\"\"\"\n    # Initialize model parameters\n    init_params = model.init(rng_key, jnp.ones((1, 1), dtype=jnp.int32))\n\n    # Create learning rate schedule\n    lr_schedule = optax.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=config.learning_rate,\n        warmup_steps=config.warmup_steps,\n        decay_steps=config.max_steps,\n        end_value=config.learning_rate * 0.1\n    )\n\n    # Create optimizer\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(1.0),\n        optax.adamw(\n            learning_rate=lr_schedule,\n            b1=0.9,\n            b2=0.95,\n            eps=1e-8,\n            weight_decay=config.weight_decay\n        )\n    )\n\n    # Create and return train state - ensure parameters have consistent structure\n    return train_state.TrainState.create(\n        apply_fn=model.apply,\n        params=init_params,\n        tx=optimizer\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:06:52.417100Z","iopub.execute_input":"2025-03-21T19:06:52.417467Z","iopub.status.idle":"2025-03-21T19:06:52.423851Z","shell.execute_reply.started":"2025-03-21T19:06:52.417440Z","shell.execute_reply":"2025-03-21T19:06:52.422738Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_step(state, batch, dropout_rng):\n    \"\"\"Single training step\"\"\"\n    inputs, targets = batch\n\n    # Define loss function\n    def loss_fn(params):\n        # Apply model with correct parameter structure\n        logits = state.apply_fn({\"params\": params}, inputs, deterministic=False, rngs={'dropout': dropout_rng})\n\n        # Reshape for cross entropy\n        logits = logits.reshape(-1, logits.shape[-1])\n        targets_flat = targets.reshape(-1)\n\n        # Compute cross entropy loss\n        loss = optax.softmax_cross_entropy_with_integer_labels(\n            logits, targets_flat\n        ).mean()\n\n        return loss\n\n    # Get gradients - make sure we're getting gradients for the actual parameters\n    grad_fn = jax.value_and_grad(loss_fn)\n\n    # Check param structure and extract the inner params if needed\n    if \"params\" in state.params:\n        actual_params = state.params[\"params\"]\n    else:\n        actual_params = state.params\n\n    loss, grads = grad_fn(actual_params)\n\n    # Now wrap the gradients in the same structure as state.params for apply_gradients\n    if \"params\" in state.params:\n        wrapped_grads = {\"params\": grads}\n    else:\n        wrapped_grads = grads\n\n    # Update state with correctly structured gradients\n    new_state = state.apply_gradients(grads=wrapped_grads)\n\n    return new_state, loss\n\n# JIT-compiled training step for efficiency\ntrain_step_jit = jax.jit(train_step)\n\n# FIXED: Define pmapped training step with improved dropout RNG handling\ndef train_step_pmap_wrapper(state, batch, dropout_rng):\n    # Wrapper for consistency when pmapping\n    return train_step(state, batch, dropout_rng)\n\ntrain_step_pmap = jax.pmap(train_step_pmap_wrapper, axis_name='batch')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:07:21.912694Z","iopub.execute_input":"2025-03-21T19:07:21.913032Z","iopub.status.idle":"2025-03-21T19:07:21.923188Z","shell.execute_reply.started":"2025-03-21T19:07:21.913005Z","shell.execute_reply":"2025-03-21T19:07:21.921719Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def evaluate(model_apply_fn, params, eval_data, config, num_batches=10):\n    \"\"\"Evaluate model on validation data\"\"\"\n    key = random.PRNGKey(42)\n    total_loss = 0.0\n\n    for i in range(num_batches):\n        key, batch_key = random.split(key)\n        inputs, targets = get_batch(batch_key, eval_data, config)\n\n        # Forward pass\n        # Check if params already has a 'params' key\n        if 'params' in params:\n            logits = model_apply_fn(params, inputs, deterministic=True)\n        else:\n            logits = model_apply_fn({'params': params}, inputs, deterministic=True)\n\n        # Reshape for cross entropy\n        logits = logits.reshape(-1, logits.shape[-1])\n        targets = targets.reshape(-1)\n\n        # Compute cross entropy loss\n        loss = optax.softmax_cross_entropy_with_integer_labels(\n            logits, targets\n        ).mean()\n\n        total_loss += loss\n\n    avg_loss = total_loss / num_batches\n    perplexity = jnp.exp(avg_loss)\n\n    return avg_loss, perplexity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:07:33.172642Z","iopub.execute_input":"2025-03-21T19:07:33.172985Z","iopub.status.idle":"2025-03-21T19:07:33.180610Z","shell.execute_reply.started":"2025-03-21T19:07:33.172959Z","shell.execute_reply":"2025-03-21T19:07:33.179092Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def train_llama(config, num_epochs=5, steps_per_epoch=1000, save_every=1000):\n    \"\"\"Train LLaMA model\"\"\"\n    # Initialize TPU\n    n_devices = initialize_tpu()\n\n    # Setup model\n    model = LLaMA3(config)\n    rng_key = random.PRNGKey(42)\n\n    # Create training state\n    state = create_train_state(model, config, rng_key)\n\n    # Replicate the state across devices for multi-device training\n    if n_devices > 1:\n        state = jax.device_put_replicated(state, jax.devices())\n\n    # Prepare datasets\n    train_dataset, tokenizer = prepare_datasets(config)\n\n    # Create checkpoint directory with absolute path\n    checkpoint_dir = os.path.abspath(\"llama_checkpoints\")\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    # Setup Orbax checkpointer\n    checkpointer = ocp.PyTreeCheckpointer()\n    options = ocp.CheckpointManagerOptions(\n        max_to_keep=3,\n        create=True\n    )\n    checkpoint_manager = ocp.CheckpointManager(\n        checkpoint_dir, \n        checkpointer, \n        options\n    )\n\n    # Training loop\n    rng_key = random.PRNGKey(0)\n    step = 0\n    total_steps = num_epochs * steps_per_epoch\n\n    print(f\"Starting training for {num_epochs} epochs ({total_steps} steps)\")\n\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n\n        for step_in_epoch in tqdm(range(steps_per_epoch), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            # Get a batch of data\n            rng_key, batch_key = random.split(rng_key)\n            batch_data = get_batch(batch_key, train_dataset, config)\n\n            # Training step\n            if n_devices > 1:\n                # Calculate per-device batch size\n                per_device_batch = config.batch_size // n_devices\n                if per_device_batch == 0:\n                    raise ValueError(f\"Batch size {config.batch_size} is too small for {n_devices} devices\")\n\n                # Reshape batch data for multi-device training\n                inputs, targets = batch_data\n\n                # Reshape to (n_devices, per_device_batch, seq_len)\n                inputs = inputs.reshape((n_devices, per_device_batch, inputs.shape[1]))\n                targets = targets.reshape((n_devices, per_device_batch, targets.shape[1]))\n\n                batch_data = (inputs, targets)\n\n                # Create per-device RNG keys with proper shape for pmap\n                rng_key, dropout_keys = create_device_rng_keys(rng_key, n_devices)\n\n                # Apply pmapped training step\n                state, loss = train_step_pmap(state, batch_data, dropout_keys)\n\n                # Average loss across devices\n                loss = jnp.mean(loss)\n            else:\n                # Single device training\n                rng_key, dropout_key = random.split(rng_key)\n                state, loss = train_step_jit(state, batch_data, dropout_key)\n\n            epoch_loss += loss\n            step += 1\n\n            # Log progress\n            if step % 100 == 0:\n                print(f\"Step {step}/{total_steps}, Loss: {loss:.4f}\")\n\n            # Save checkpoint\n            if step % save_every == 0:\n                if n_devices > 1:\n                    # For multi-device, save only the first copy\n                    save_state = jax.tree.map(lambda x: x[0], state)\n                else:\n                    save_state = state\n\n                # Save checkpoint using Orbax\n                save_args = orbax_utils.save_args_from_target(save_state)\n                checkpoint_manager.save(step, save_state, save_kwargs={'save_args': save_args})\n                print(f\"Checkpoint saved at step {step}\")\n\n                # Generate sample text\n                if n_devices > 1:\n                    sample_params = jax.tree.map(lambda x: x[0], state.params)\n                else:\n                    sample_params = state.params\n\n                prompt = tokenizer.encode(\"Once upon a time\").ids\n                prompt_tensor = jnp.array([prompt])\n\n                sample_rng = random.PRNGKey(step)\n                # Check if sample_params already has a 'params' key\n                if 'params' in sample_params:\n                    generated = model.apply(\n                        sample_params,\n                        prompt_tensor,\n                        max_new_tokens=50,\n                        rng_key=sample_rng,\n                        temperature=config.temperature,\n                        top_k=config.top_k,\n                        top_p=config.top_p,\n                        method=model.generate\n                    )\n                else:\n                    generated = model.apply(\n                        {\"params\": sample_params},\n                        prompt_tensor,\n                        max_new_tokens=50,\n                        rng_key=sample_rng,\n                        temperature=config.temperature,\n                        top_k=config.top_k,\n                        top_p=config.top_p,\n                        method=model.generate\n                    )\n                generated_text = tokenizer.decode(generated[0].tolist())\n                print(f\"\\nSample generation at step {step}:\\n{generated_text}\\n\")\n\n        # End of epoch\n        avg_epoch_loss = epoch_loss / steps_per_epoch\n        print(f\"Epoch {epoch+1} complete. Average loss: {avg_epoch_loss:.4f}\")\n\n        # Evaluate on validation set\n        if n_devices > 1:\n            eval_params = jax.tree.map(lambda x: x[0], state.params)\n        else:\n            eval_params = state.params\n\n        # Validation loss and perplexity\n        val_loss, perplexity = evaluate(model.apply, eval_params, train_dataset, config)\n        print(f\"Validation Loss: {val_loss:.4f}, Perplexity: {perplexity:.2f}\")\n\n    # Save final model\n    if n_devices > 1:\n        final_state = jax.tree.map(lambda x: x[0], state)\n    else:\n        final_state = state\n\n    # Save final checkpoint using Orbax\n    save_args = orbax_utils.save_args_from_target(final_state)\n    checkpoint_manager.save(\n        total_steps, \n        final_state, \n        save_kwargs={'save_args': save_args}\n    )\n    print(\"Training complete. Final model saved.\")\n\n    return final_state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:07:52.705185Z","iopub.execute_input":"2025-03-21T19:07:52.705556Z","iopub.status.idle":"2025-03-21T19:07:52.724560Z","shell.execute_reply.started":"2025-03-21T19:07:52.705528Z","shell.execute_reply":"2025-03-21T19:07:52.723061Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def generate_text(model, params, tokenizer, prompt, max_new_tokens=100, temperature=0.8):\n    \"\"\"Generate text from a prompt\"\"\"\n    # Ensure prompt is a string\n    if not isinstance(prompt, str):\n        prompt = str(prompt)\n\n    prompt_tokens = tokenizer.encode(prompt).ids\n    prompt_tensor = jnp.array([prompt_tokens])\n\n    rng_key = random.PRNGKey(0)\n    # Check if params already has a 'params' key\n    if 'params' in params:\n        generated = model.apply(\n            params,\n            prompt_tensor,\n            max_new_tokens=max_new_tokens,\n            rng_key=rng_key,\n            temperature=temperature,\n            top_k=40,\n            top_p=0.95,\n            method=model.generate\n        )\n    else:\n        generated = model.apply(\n            {\"params\": params},\n            prompt_tensor,\n            max_new_tokens=max_new_tokens,\n            rng_key=rng_key,\n            temperature=temperature,\n            top_k=40,\n            top_p=0.95,\n            method=model.generate\n        )\n\n    # Convert jnp array to Python list before decoding\n    generated_text = tokenizer.decode(generated[0].tolist())\n    return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:08:10.208588Z","iopub.execute_input":"2025-03-21T19:08:10.208920Z","iopub.status.idle":"2025-03-21T19:08:10.215768Z","shell.execute_reply.started":"2025-03-21T19:08:10.208896Z","shell.execute_reply":"2025-03-21T19:08:10.214655Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def get_batch(key, data, config):\n    \"\"\"Create a batch of data for training\"\"\"\n    batch_size = config.batch_size\n    seq_len = config.max_seq_len\n\n    # Generate random starting indices\n    total_tokens = len(data[\"input_ids\"]) - seq_len - 1  # -1 for target shifting\n\n    # Make sure we have enough tokens\n    if total_tokens <= 0:\n        raise ValueError(f\"Not enough tokens in dataset. Found {len(data['input_ids'])}, need at least {seq_len + 2}\")\n\n    # Generate batch_size random starting points\n    ix = random.randint(key, (batch_size,), 0, total_tokens)\n\n    # Create input and target sequences\n    x = jnp.stack([jnp.array(data[\"input_ids\"][i:i+seq_len]) for i in ix])\n    y = jnp.stack([jnp.array(data[\"input_ids\"][i+1:i+seq_len+1]) for i in ix])\n\n    return x, y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_checkpoint(checkpoint_dir, step=None):\n    \"\"\"Load a checkpoint from the given directory\"\"\"\n    checkpoint_dir = os.path.abspath(checkpoint_dir)\n    \n    # Setup Orbax checkpointer\n    checkpointer = ocp.PyTreeCheckpointer()\n    options = ocp.CheckpointManagerOptions(create=False)\n    checkpoint_manager = ocp.CheckpointManager(\n        checkpoint_dir, \n        checkpointer, \n        options\n    )\n    \n    # Get the latest step if none specified\n    if step is None:\n        step = checkpoint_manager.latest_step()\n        if step is None:\n            raise ValueError(f\"No checkpoints found in {checkpoint_dir}\")\n    \n    # Create a dummy state to restore structure\n    model = LLaMA3(LLaMAConfig())\n    rng_key = random.PRNGKey(0)\n    dummy_state = create_train_state(model, LLaMAConfig(), rng_key)\n    \n    # Restore checkpoint\n    restored_state = checkpoint_manager.restore(step, dummy_state)\n    print(f\"Restored checkpoint from step {step}\")\n    \n    return restored_state, step","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:08:27.759798Z","iopub.execute_input":"2025-03-21T19:08:27.760142Z","iopub.status.idle":"2025-03-21T19:08:27.766555Z","shell.execute_reply.started":"2025-03-21T19:08:27.760102Z","shell.execute_reply":"2025-03-21T19:08:27.765425Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def prepare_datasets(config):\n    \"\"\"Load and prepare datasets for LLaMA-like LLM training with flexible dataset support\"\"\"\n    from datasets import load_dataset\n    from tokenizers import SentencePieceUnigramTokenizer, Tokenizer, models, pre_tokenizers\n    import os\n    \n    # Load dataset based on configuration\n    if hasattr(config, 'dataset_name') and config.dataset_name:\n        # Load from Hugging Face datasets\n        if hasattr(config, 'dataset_config') and config.dataset_config:\n            dataset = load_dataset(config.dataset_name, config.dataset_config, split=config.split)\n        else:\n            dataset = load_dataset(config.dataset_name, split=config.split)\n    elif hasattr(config, 'dataset_path') and config.dataset_path:\n        # Load from local files\n        dataset = load_dataset(\n            'json' if config.dataset_path.endswith('.json') else 'text', \n            data_files=config.dataset_path, \n            split=config.split\n        )\n    else:\n        # Default to tiny_shakespeare as fallback\n        dataset = load_dataset(\"karpathy/tiny_shakespeare\", split=\"train\")\n    \n    # Print the dataset structure\n    print(f\"Dataset loaded: {config.dataset_name if hasattr(config, 'dataset_name') else 'local'}\")\n    print(\"Dataset structure:\", list(dataset.features.keys()))\n    print(f\"Dataset size: {len(dataset)} examples\")\n    \n    # Determine text column\n    text_column = getattr(config, 'text_column', None)\n    if not text_column:\n        column_names = list(dataset.features.keys())\n        text_column = next((col for col in ['text', 'content', 'document'] \n                          if col in column_names), column_names[0])\n    print(f\"Using '{text_column}' as the text column\")\n    \n    # Initialize tokenizer - either load existing or train new one\n    tokenizer_path = getattr(config, 'tokenizer_path', \"llama_tokenizer.json\")\n    if os.path.exists(tokenizer_path) and getattr(config, 'use_existing_tokenizer', False):\n        print(f\"Loading existing tokenizer from {tokenizer_path}\")\n        tokenizer = Tokenizer.from_file(tokenizer_path)\n    else:\n        print(\"Training new tokenizer\")\n        \n        # Set special tokens from config or use defaults\n        special_tokens = getattr(config, 'special_tokens', [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"])\n        \n        # Ensure <unk> token is in the special tokens\n        if \"<unk>\" not in special_tokens:\n            special_tokens.append(\"<unk>\")\n            print(\"Added <unk> token to special tokens\")\n        \n        # Get sample texts for training the tokenizer\n        sample_size = min(getattr(config, 'tokenizer_sample_size', 10000), len(dataset))\n        \n        # Sample texts, handling potentially different data formats\n        sample_texts = []\n        for example in dataset.select(range(sample_size)):\n            text = example.get(text_column, \"\")\n            if isinstance(text, str) and text.strip():\n                sample_texts.append(text)\n            elif isinstance(text, list) and all(isinstance(t, str) for t in text):\n                sample_texts.extend([t for t in text if t.strip()])\n        \n        # Create and train the tokenizer\n        tokenizer = SentencePieceUnigramTokenizer()\n        tokenizer.train_from_iterator(\n            sample_texts,\n            vocab_size=config.vocab_size,\n            special_tokens=special_tokens,\n            unk_token=\"<unk>\"  # Explicitly specify the unk token\n        )\n        \n        # Save the tokenizer - make sure path is a valid string\n        tokenizer_save_path = getattr(config, 'tokenizer_save_path', tokenizer_path)\n        # Ensure the path is a valid string and not None\n        if tokenizer_save_path is None:\n            tokenizer_save_path = \"llama_tokenizer.json\"\n        \n        print(f\"Saving tokenizer to {tokenizer_save_path}\")\n        tokenizer.save(tokenizer_save_path)\n        print(f\"Tokenizer saved to {tokenizer_save_path}\")\n    \n    # Define tokenize function with handling for unknown tokens\n    def tokenize_function(example):\n        try:\n            # Handle different text formats (string, list of strings, etc.)\n            text = example.get(text_column, \"\")\n            \n            if isinstance(text, str) and text.strip():\n                # Use encode with add_special_tokens=False to avoid adding special tokens automatically\n                encoded = tokenizer.encode(text)\n                return {\"input_ids\": encoded.ids}\n            elif isinstance(text, list) and all(isinstance(t, str) for t in text):\n                # Handle list of strings (e.g., for dialogue datasets)\n                tokens = []\n                for t in text:\n                    if t.strip():\n                        encoded = tokenizer.encode(t)\n                        tokens.extend(encoded.ids)\n                        if getattr(config, 'add_eos_between_segments', False):\n                            # Add EOS token between segments if configured\n                            eos_id = tokenizer.token_to_id(\"<eos>\")\n                            if eos_id is not None:\n                                tokens.append(eos_id)\n                return {\"input_ids\": tokens}\n            \n            return {\"input_ids\": []}\n        except Exception as e:\n            print(f\"Error tokenizing example: {e}\")\n            # Return empty list for problematic examples\n            return {\"input_ids\": []}\n    \n    # Tokenize dataset with progress reporting\n    print(\"Tokenizing dataset...\")\n    tokenized_dataset = dataset.map(\n        tokenize_function,\n        remove_columns=[col for col in dataset.column_names if col != text_column] \n        if getattr(config, 'keep_text_column', False) else dataset.column_names,\n        batched=False,  # Process one example at a time for better error handling\n        desc=\"Tokenizing\"\n    )\n    \n    # Filter out empty examples\n    original_size = len(tokenized_dataset)\n    tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) > 0)\n    filtered_size = len(tokenized_dataset)\n    if original_size != filtered_size:\n        print(f\"Filtered out {original_size - filtered_size} empty examples\")\n    \n    # Process based on the specified format\n    format_type = getattr(config, 'format_type', \"flat\")\n    \n    if format_type == \"flat\":\n        # Flatten all token sequences into one long sequence\n        all_tokens = []\n        for example in tokenized_dataset:\n            all_tokens.extend(example[\"input_ids\"])\n            \n            # Add EOS token between examples if configured\n            if getattr(config, 'add_eos_between_examples', False):\n                eos_id = tokenizer.token_to_id(\"<eos>\")\n                if eos_id is not None:\n                    all_tokens.append(eos_id)\n                    \n        print(f\"Created flattened dataset with {len(all_tokens)} tokens\")\n        return {\"input_ids\": all_tokens}, tokenizer\n    \n    elif format_type == \"chunked\":\n        # Create fixed-length chunks of tokens for training\n        chunk_size = getattr(config, 'chunk_size', 512)\n        overlap = getattr(config, 'chunk_overlap', 0)\n        \n        chunked_datasets = []\n        for example in tokenized_dataset:\n            tokens = example[\"input_ids\"]\n            for i in range(0, len(tokens) - chunk_size + 1, chunk_size - overlap):\n                chunk = tokens[i:i + chunk_size]\n                if len(chunk) == chunk_size:  # Only keep full-sized chunks\n                    chunked_datasets.append({\"input_ids\": chunk})\n        \n        print(f\"Created {len(chunked_datasets)} chunks of size {chunk_size}\")\n        return chunked_datasets, tokenizer\n    \n    else:\n        # Keep dataset as is (one example per entry)\n        print(f\"Using dataset with {len(tokenized_dataset)} separate examples\")\n        return tokenized_dataset, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:23:17.947768Z","iopub.execute_input":"2025-03-21T19:23:17.948149Z","iopub.status.idle":"2025-03-21T19:23:17.969267Z","shell.execute_reply.started":"2025-03-21T19:23:17.948109Z","shell.execute_reply":"2025-03-21T19:23:17.968195Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"config = LLaMAConfig()\n# Create checkpoint directory with absolute path\ncheckpoint_dir = os.path.abspath(\"llama_checkpoints\")\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Train the model\nfinal_state = train_llama(config, num_epochs=5, steps_per_epoch=10,save_every= 10)\n\n# Generate some text\nmodel = LLaMA3(config)\ndataset, tokenizer = prepare_datasets(config)\n\nprompt = \"In a distant galaxy\"\ngenerated_text = generate_text(model, final_state.params, tokenizer, prompt)\n\nprint(\"\\nGenerated text:\")\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:23:19.954245Z","iopub.execute_input":"2025-03-21T19:23:19.954614Z"}},"outputs":[{"name":"stdout","text":"Found 8 JAX devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\nTPU devices detected. Setting up for distributed training.\nDataset loaded: wikitext\nDataset structure: ['text']\nDataset size: 36718 examples\nUsing 'text' as the text column\nTraining new tokenizer\n\n\nSaving tokenizer to llama_tokenizer.json\nTokenizer saved to llama_tokenizer.json\nTokenizing dataset...\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing: 100%|██████████| 36718/36718 [00:10<00:00, 3586.19 examples/s]\nFilter: 100%|██████████| 36718/36718 [00:01<00:00, 24067.68 examples/s]\n","output_type":"stream"},{"name":"stdout","text":"Filtered out 12951 empty examples\nCreated flattened dataset with 3228097 tokens\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate.\n","output_type":"stream"},{"name":"stdout","text":"Starting training for 5 epochs (50 steps)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5:  90%|█████████ | 9/10 [00:20<00:00,  1.55it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at step 10\n\nSample generation at step 10:\nOnce upon a time Lu Campbelnication Chronicle informal chamberoticgrass Forever chromo León Brooke faction Chic 10 CB Rad illness east explode Mennonite bridgehead Rand installment fel gam extensive month herodgesuniversit composition bloodWork website allegeontemporar Th Budd efficien Movie Indo mobili Denn glyph strong flood Mix\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 10/10 [00:45<00:00,  4.52s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 complete. Average loss: 10.4922\nValidation Loss: 10.4245, Perplexity: 33673.60\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5:  90%|█████████ | 9/10 [00:03<00:00,  2.85it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at step 20\n\nSample generation at step 20:\nOnce upon a time stair Nc devou Dramaotte near afterzong come Kon Jesuit Toddicive naturali Patriot deemface Wrappsupplycast 800 Undermailengage news Thak grayny negative Finalediment Four Hero Hisrelevan Clin festivbe ก fighter striking ambassador speed weaken basinjosi Abraham\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 10/10 [00:18<00:00,  1.89s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 complete. Average loss: 10.3284\nValidation Loss: 10.1869, Perplexity: 26552.34\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5:  90%|█████████ | 9/10 [00:03<00:00,  2.89it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at step 30\n\nSample generation at step 30:\nOnce upon a time use transitionprimari origin judgement geographicalday feti extravagan in pharmac Berardiulgar Op Vert microscop Qui19 Age televis Heli chalkmb automobileers Balk Hutch old consiste Iron Jackson Brown Paradis undul arrangement substitu Vir Cooper Shawn theropod commissionerule34 Eff grille Cre recurren\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 10/10 [00:19<00:00,  1.93s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 complete. Average loss: 10.0531\nValidation Loss: 9.8882, Perplexity: 19695.82\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5:  90%|█████████ | 9/10 [00:03<00:00,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at step 40\n\nSample generation at step 40:\nOnce upon a timedayac antibioticbase nucleophilic vertical 193 def child body spring Waldenabbar Rhyonly th commission Tower teacher keyolmenworkernburg Bate Aircraft David Ca Chaseytimid song Christmasrate oppos Pre commissionere flightroblemday view Yon resident ColightphanwideCarcole Play\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 10/10 [00:26<00:00,  2.70s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 complete. Average loss: 9.7504\nValidation Loss: 9.5693, Perplexity: 14318.66\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5:  90%|█████████ | 9/10 [00:03<00:00,  2.87it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at step 50\n\nSample generation at step 50:\nOnce upon a timedway listen deportdu rareengagearily Alphabe Listsellal camprgin Plagi Span clergy cry drawclosureinterpretelips 16 piece Din structuralgene auto RAtrox communit Chia And enormous relate melodrama Sabre suitabl demolition accus Sincsed cyclo appreciate approach Jeffer believe LeoniLine\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 10/10 [00:27<00:00,  2.74s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 complete. Average loss: 9.4183\nValidation Loss: 9.2760, Perplexity: 10678.33\nTraining complete. Final model saved.\nDataset loaded: wikitext\nDataset structure: ['text']\nDataset size: 36718 examples\nUsing 'text' as the text column\nTraining new tokenizer\n\n\nSaving tokenizer to llama_tokenizer.json\nTokenizer saved to llama_tokenizer.json\nTokenizing dataset...\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing: 100%|██████████| 36718/36718 [00:13<00:00, 2686.26 examples/s]\nFilter: 100%|██████████| 36718/36718 [00:01<00:00, 23814.66 examples/s]\n","output_type":"stream"},{"name":"stdout","text":"Filtered out 12951 empty examples\nCreated flattened dataset with 3228097 tokens\n","output_type":"stream"}],"execution_count":null}]}