 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Gemma: A JAX/Flax Implementation Tutorial\n",
    "\n",
    "This notebook demonstrates how to implement a smaller version of the Gemma language model using JAX and Flax. We'll build the model step by step, explaining key concepts along the way.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Dependencies](#setup)\n",
    "2. [Model Configuration](#config)\n",
    "3. [Core Model Components](#components)\n",
    "4. [Training Infrastructure](#training)\n",
    "5. [Training Loop and Generation](#generation)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies <a name=\"setup\"></a>\n",
    "\n",
    "First, let's import all necessary libraries. We'll be using:\n",
    "- JAX for numerical computations and automatic differentiation\n",
    "- Flax for neural network layers\n",
    "- Optax for optimization\n",
    "- Other utilities for data handling and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, lax, vmap\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from flax.training import train_state, orbax_utils\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "# Check for TPU and set environment\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'tpu'\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "print(\"JAX devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Configuration <a name=\"config\"></a>\n",
    "\n",
    "The `GemmaConfig` class defines all hyperparameters for our model. We're using a smaller version of the original Gemma model to make it more manageable for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GemmaConfig:\n",
    "    \"\"\"Configuration for Gemma model\"\"\"\n",
    "    vocab_size: int = 32000\n",
    "    dim: int = 256  # Reduced from 512 to save memory\n",
    "    n_layers: int = 4  # Reduced from 8 to save memory\n",
    "    n_heads: int = 4  # Reduced from 8 to save memory\n",
    "    n_kv_heads: int = 1  # For multi-query attention, use 1 KV head\n",
    "    max_seq_len: int = 512  # Reduced from 2048 to save memory\n",
    "    dropout_rate: float = 0.0  # Dropout rate\n",
    "\n",
    "    # RoPE settings\n",
    "    rope_theta: float = 10000.0  # Base for rotary embeddings\n",
    "\n",
    "    # Training settings\n",
    "    batch_size: int = 16  # Reduced from 16 to save memory\n",
    "    learning_rate: float = 3e-4\n",
    "    weight_decay: float = 0.1\n",
    "    warmup_steps: int = 100\n",
    "    max_steps: int = 10000\n",
    "\n",
    "    # Generation settings\n",
    "    temperature: float = 0.8\n",
    "    top_k: int = 40\n",
    "    top_p: float = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Model Components <a name=\"components\"></a>\n",
    "\n",
    "### 3.1 RMS Normalization\n",
    "Root Mean Square Layer Normalization is a simpler alternative to traditional Layer Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
    "    dim: int\n",
    "    eps: float = 1e-5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        weight = self.param('weight', nn.initializers.ones, (self.dim,))\n",
    "        variance = jnp.mean(jnp.square(x), axis=-1, keepdims=True)\n",
    "        x = x * jnp.reciprocal(jnp.sqrt(variance + self.eps))\n",
    "        return x * weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Rotary Position Embeddings (RoPE)\n",
    "RoPE is a method for encoding positional information into the key and query vectors of the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def precompute_freqs_cis(dim: int, max_seq_len: int, theta: float = 10000.0):\n",
    "    \"\"\"Precompute the frequency tensor for complex exponentials (rotary embeddings).\"\"\"\n",
    "    freqs = 1.0 / (theta ** (jnp.arange(0, dim // 2, dtype=jnp.float32) / dim))\n",
    "    t = jnp.arange(max_seq_len, dtype=jnp.float32)\n",
    "    freqs = jnp.outer(t, freqs)\n",
    "    return jnp.complex64(jnp.exp(1j * freqs))\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "    \"\"\"Apply rotary embeddings to the query and key tensors.\"\"\"\n",
    "    xq_r = jnp.reshape(xq, (*xq.shape[:-1], -1, 2))\n",
    "    xk_r = jnp.reshape(xk, (*xk.shape[:-1], -1, 2))\n",
    "    \n",
    "    xq_complex = jnp.complex64(xq_r[..., 0] + 1j * xq_r[..., 1])\n",
    "    xk_complex = jnp.complex64(xk_r[..., 0] + 1j * xk_r[..., 1])\n",
    "    \n",
    "    freqs_cis = jnp.reshape(freqs_cis, (1, freqs_cis.shape[0], 1, freqs_cis.shape[1]))\n",
    "    \n",
    "    xq_out = xq_complex * freqs_cis\n",
    "    xk_out = xk_complex * freqs_cis\n",
    "    \n",
    "    xq = jnp.stack([jnp.real(xq_out), jnp.imag(xq_out)], axis=-1).reshape(xq.shape)\n",
    "    xk = jnp.stack([jnp.real(xk_out), jnp.imag(xk_out)], axis=-1).reshape(xk.shape)\n",
    "    \n",
    "    return xq, xk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Flash Attention\n",
    "An optimized implementation of the attention mechanism for better memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@partial(jax.jit)\n",
    "def flash_attention(q, k, v, mask=None, scale=None):\n",
    "    \"\"\"Optimized attention implementation\"\"\"\n",
    "    if scale is None:\n",
    "        scale = 1.0 / jnp.sqrt(q.shape[-1])\n",
    "\n",
    "    scores = jnp.einsum('bhid,bhjd->bhij', q, k) * scale\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores + mask\n",
    "\n",
    "    scores_max = jnp.max(scores, axis=-1, keepdims=True)\n",
    "    scores = scores - lax.stop_gradient(scores_max)\n",
    "\n",
    "    attn_weights = jnp.exp(scores)\n",
    "    attn_weights = attn_weights / jnp.sum(attn_weights, axis=-1, keepdims=True)\n",
    "\n",
    "    output = jnp.einsum('bhij,bhjd->bhid', attn_weights, v)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Multi-Query Attention\n",
    "The attention mechanism with shared key and value heads across query heads for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GemmaCausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-query attention (single KV head shared across all query heads)\"\"\"\n",
    "    config: GemmaConfig\n",
    "\n",
    "    def setup(self):\n",
    "        config = self.config\n",
    "        dim = config.dim\n",
    "        n_heads = config.n_heads\n",
    "        n_kv_heads = config.n_kv_heads\n",
    "        head_dim = dim // n_heads\n",
    "\n",
    "        self.wq = nn.Dense(n_heads * head_dim,\n",
    "                          kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n",
    "        self.wk = nn.Dense(n_kv_heads * head_dim,\n",
    "                          kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n",
    "        self.wv = nn.Dense(n_kv_heads * head_dim,\n",
    "                          kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n",
    "        self.wo = nn.Dense(dim,\n",
    "                          kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n",
    "\n",
    "    def __call__(self, x, freqs_cis, mask=None, deterministic=True):\n",
    "        B, T, C = x.shape\n",
    "        config = self.config\n",
    "        n_heads = config.n_heads\n",
    "        n_kv_heads = config.n_kv_heads\n",
    "        head_dim = C // n_heads\n",
    "\n",
    "        q = self.wq(x).reshape(B, T, n_heads, head_dim)\n",
    "        k = self.wk(x).reshape(B, T, n_kv_heads, head_dim)\n",
    "        v = self.wv(x).reshape(B, T, n_kv_heads, head_dim)\n",
    "\n",
    "        q, k = apply_rotary_emb(q, k, freqs_cis[:T])\n",
    "\n",
    "        if n_heads > n_kv_heads:\n",
    "            k = jnp.repeat(k, n_heads // n_kv_heads, axis=2)\n",
    "            v = jnp.repeat(v, n_heads // n_kv_heads, axis=2)\n",
    "\n",
    "        q, k, v = map(lambda x: jnp.swapaxes(x, 1, 2), (q, k, v))\n",
    "        output = flash_attention(q, k, v, mask)\n",
    "        output = jnp.swapaxes(output, 1, 2).reshape(B, T, -1)\n",
    "        output = self.wo(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 MLP with GeGLU Activation\n",
    "The feed-forward network using GeGLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def geglu(x, w1, w2, w3):\n",
    "    \"\"\"GeGLU activation function using Flax modules\"\"\"\n",
    "    return w2(jax.nn.gelu(w3(x), approximate=True) * w1(x))\n",
    "\n",
    "class GemmaMLP(nn.Module):\n",
    "    \"\"\"Feed-forward network with GeGLU activation\"\"\"\n",
    "    config: GemmaConfig\n",
    "\n",
    "    def setup(self):\n",
    "        dim = self.config.dim\n",
    "        hidden_dim = 4 * dim\n",
    "\n",
    "        self.w1 = nn.Dense(hidden_dim,\n",
    "                         kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n",
    "        self.w2 = nn.Dense(dim,\n",
    "                         kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n",
    "        self.w3 = nn.Dense(hidden_dim,\n",
    "                         kernel_init=nn.initializers.variance_scaling(1.0, 'fan_in', 'normal'))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return geglu(x, self.w1, self.w2, self.w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Transformer Block\n",
    "The complete transformer block combining attention and MLP layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GemmaBlock(nn.Module):\n",
    "    \"\"\"Gemma transformer block with pre-normalization\"\"\"\n",
    "    config: GemmaConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention_norm = RMSNorm(self.config.dim)\n",
    "        self.attention = GemmaCausalSelfAttention(self.config)\n",
    "        self.ffn_norm = RMSNorm(self.config.dim)\n",
    "        self.ffn = GemmaMLP(self.config)\n",
    "        self.dropout = nn.Dropout(self.config.dropout_rate)\n",
    "\n",
    "    def __call__(self, x, freqs_cis, mask=None, deterministic=True):\n",
    "        attn_input = self.attention_norm(x)\n",
    "        attn_output = self.attention(attn_input, freqs_cis, mask, deterministic)\n",
    "        h = x + self.dropout(attn_output, deterministic=deterministic)\n",
    "\n",
    "        ffn_input = self.ffn_norm(h)\n",
    "        ffn_output = self.ffn(ffn_input)\n",
    "        out = h + self.dropout(ffn_output, deterministic=deterministic)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Complete Gemma Model\n",
    "The full language model combining all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Gemma(nn.Module):\n",
    "    \"\"\"Gemma language model\"\"\"\n",
    "    config: GemmaConfig\n",
    "\n",
    "    def setup(self):\n",
    "        config = self.config\n",
    "\n",
    "        self.token_embedding = nn.Embed(\n",
    "            config.vocab_size,\n",
    "            config.dim,\n",
    "            embedding_init=nn.initializers.normal(stddev=0.02)\n",
    "        )\n",
    "\n",
    "        self.blocks = [GemmaBlock(config) for _ in range(config.n_layers)]\n",
    "        self.norm_f = RMSNorm(config.dim)\n",
    "        self.lm_head = nn.Dense(\n",
    "            config.vocab_size,\n",
    "            kernel_init=nn.initializers.normal(stddev=0.02),\n",
    "            use_bias=False\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            config.dim // config.n_heads,\n",
    "            config.max_seq_len,\n",
    "            config.rope_theta\n",
    "        )\n",
    "\n",
    "    def __call__(self, input_ids, deterministic=True):\n",
    "        B, T = input_ids.shape\n",
    "\n",
    "        mask = jnp.tril(\n",
    "            jnp.ones((self.config.max_seq_len, self.config.max_seq_len))\n",
    "        )\n",
    "        mask = jnp.where(mask == 0, jnp.finfo(jnp.float32).min, 0.0)\n",
    "        mask = mask[None, None, :T, :T]\n",
    "\n",
    "        h = self.token_embedding(input_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            h = block(h, self.freqs_cis, mask, deterministic)\n",
    "\n",
    "        h = self.norm_f(h)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Infrastructure <a name=\"training\"></a>\n",
    "\n",
    "Let's set up the training infrastructure including data preparation and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_train_state(model, config, rng_key):\n",
    "    \"\"\"Create initial training state\"\"\"\n",
    "    init_params = model.init(rng_key, jnp.ones((1, 1), dtype=jnp.int32))\n",
    "\n",
    "    lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=config.learning_rate,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        decay_steps=config.max_steps,\n",
    "        end_value=config.learning_rate * 0.1\n",
    "    )\n",
    "\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(\n",
    "            learning_rate=lr_schedule,\n",
    "            b1=0.9,\n",
    "            b2=0.95,\n",
    "            eps=1e-8,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=init_params,\n",
    "        tx=optimizer\n",
    "    )\n",
    "\n",
    "def prepare_datasets(config):\n",
    "    \"\"\"Load and prepare datasets\"\"\"\n",
    "    wiki_dataset = load_dataset(\"karpathy/tiny_shakespeare\", split=\"train\")\n",
    "    \n",
    "    tokenizer = SentencePieceUnigramTokenizer()\n",
    "    \n",
    "    text_column = \"text\"\n",
    "    sample_texts = [\n",
    "        example[text_column] for example in wiki_dataset.select(range(min(10000, len(wiki_dataset))))\n",
    "        if isinstance(example[text_column], str) and example[text_column].strip()\n",
    "    ]\n",
    "\n",
    "    tokenizer.train_from_iterator(\n",
    "        sample_texts,\n",
    "        vocab_size=config.vocab_size,\n",
    "        special_tokens=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
    "    )\n",
    "\n",
    "    tokenizer.save(\"gemma_tokenizer.json\")\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        text = example[text_column]\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            tokens = tokenizer.encode(text).ids\n",
    "            return {\"input_ids\": tokens}\n",
    "        return {\"input_ids\": []}\n",
    "\n",
    "    tokenized_dataset = wiki_dataset.map(\n",
    "        tokenize_function,\n",
    "        remove_columns=list(wiki_dataset.features.keys()),\n",
    "        batched=False\n",
    "    )\n",
    "\n",
    "    tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) > 0)\n",
    "\n",
    "    all_tokens = []\n",
    "    for example in tokenized_dataset:\n",
    "        all_tokens.extend(example[\"input_ids\"])\n",
    "\n",
    "    return {\"input_ids\": all_tokens}, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop and Generation <a name=\"generation\"></a>\n",
    "\n",
    "Now let's implement the training loop and text generation functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_step(state, batch, dropout_rng):\n",
    "    \"\"\"Single training step\"\"\"\n",
    "    inputs, targets = batch\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(\n",
    "            {'params': params}, \n",
    "            inputs, \n",
    "            deterministic=False,\n",
    "            rngs={'dropout': dropout_rng}\n",
    "        )\n",
    "\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits, targets_flat\n",
    "        ).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params['params'])\n",
    "    new_state = state.apply_gradients(grads={'params': grads})\n",
    "\n",
    "    return new_state, {'loss': loss}\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize config and model\n",
    "    config = GemmaConfig()\n",
    "    model = Gemma(config)\n",
    "    \n",
    "    # Create training state\n",
    "    rng_key = random.PRNGKey(0)\n",
    "    state = create_train_state(model, config, rng_key)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    train_dataset, tokenizer = prepare_datasets(config)\n",
    "    \n",
    "    # Training loop (simplified for demonstration)\n",
    "    num_steps = 100\n",
    "    for step in range(num_steps):\n",
    "        # Get batch\n",
    "        rng_key, data_key, dropout_key = random.split(rng_key, 3)\n",
    "        batch = get_batch(data_key, train_dataset, config)\n",
    "        \n",
    "        # Training step\n",
    "        state, metrics = train_step(state, batch, dropout_key)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}, Loss: {metrics['loss']:.4f}\")\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated how to implement a smaller version of the Gemma language model using JAX and Flax. Key takeaways:\n",
    "\n",
    "1. The model uses modern architecture components like RMSNorm and Rotary Position Embeddings\n",
    "2. Multi-query attention helps reduce memory usage while maintaining model quality\n",
    "3. The implementation is optimized for TPU/GPU acceleration using JAX\n",
    "\n",
    "To extend this further, you could:\n",
    "- Scale up the model size\n",
    "- Implement more sophisticated training techniques\n",
    "- Add model parallel training support\n",
    "- Implement better tokenization strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}