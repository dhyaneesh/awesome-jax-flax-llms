---

# ğŸš€ **Awesome JAX & Flax LLMs**  

Welcome to [awesome-jax-flax-llms](https://github.com/dhyaneesh/awesome-jax-flax-llms), a curated collection of open-source large language model (LLM) implementations built with **JAX & Flax**. This repository provides modular, efficient, and scalable implementations of transformer-based models, optimized for **high-speed TPU/GPU training** and **efficient inference**.  

## ğŸ›  **Features**  
- âœ… **Multiple LLM architectures implemented in JAX/Flax**  
- âœ… **Optimized for TPU acceleration with JAXâ€™s XLA compiler**  
- âœ… **Highly modular & extensible codebase**  
- âœ… **Efficient training with Optax optimizers**  
- âœ… **Hugging face support to train on various datasets**  
- â³ **Fine-tuning support (Coming Soon!)**  

## ğŸ“š **Implemented Models**  

### âœ… **GPT-2 - JAX/Flax**  
A compact transformer-based language model implemented in **pure JAX/Flax**. This implementation leverages **XLA optimizations** for parallelism, making it efficient on **TPUs and GPUs**. It serves as the foundation for exploring JAX-based language modeling.  

ğŸ“Œ *Notebook: `models/gpt-2/gpt2_in_jax.ipynb`*  
ğŸ“Œ *Script: `models/gpt-2/train.py`*  

### â³ **Llama 2 - JAX (WIP)**  
An effort to bring **Metaâ€™s Llama 2** to JAX, focusing on **memory-efficient attention** and **scalability for TPU-based pretraining**. This implementation aims to push **large-scale inference and training** in JAX environments.  

### â³ **Llama 3 - JAX (WIP)**  
An extension of the Llama series, incorporating **state-of-the-art optimizations** in JAX for handling **longer context windows** and **reduced memory footprint** with precision tuning.  


### ğŸ“… **Mistral - JAX (Coming Soon)**  
A high-performance implementation of the **Mistral architecture**, featuring **dense & sparse mixture-of-expert layers**. This model will showcase **advanced TPU utilization** and optimized autoregressive decoding.  


---

## ğŸ“– **Usage**  

### **Recommended Environment: Google Colab**  
These models are best run in **Google Colab**, which provides **free TPU support** for optimal performance.  

### **Running the Notebooks**  
Each model has its own Jupyter notebook. Navigate to the respective directories and open the notebook in **Google Colab** to explore the implementations.  

---

## ğŸ”¥ **Next Steps**  
- ğŸ— **Fine-tuning Support**: Enabling training on custom datasets.  
- âš¡ **Larger Model Implementations**: Expanding the repo with more LLMs.  
- ğŸ† **Performance Optimizations**: Enhancing TPU inference efficiency.  

## ğŸ“œ **License**  
This project is licensed under the **GNU General Public License v3.0 (GPL-3.0)**. See the [LICENSE](LICENSE) file for details.  

---

ğŸ’¡ *Contributions are welcome! Feel free to submit issues and pull requests.*  

---
